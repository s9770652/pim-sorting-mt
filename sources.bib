@article{Shell1959AHS,
	author={Donald Lewis Shell},
	title={A high\hyphen{}speed sorting procedure},
	journaltitle={Communications of the ACM},
	date={1959-07-01},
	editor={Alan J. Perlis},
	volume={2},
	issue={7},
	pages={30-32},
	doi={10.1145/368370.368387},
}

@misc{skean2023optimization,
	author={Oscar Skean and Richard Ehrenborg and Jerzy W. Jaromczyk},
	title={Optimization Perspectives on Shellsort},
	date={2023-01-01},
	eprint={2301.00316v1},
	archivePrefix={arXiv},
	primaryClass={cs.DS},
}

@InProceedings{10.1007/3-540-44669-9_12,
	author={Marcin Ciura},
	editor={Freivalds, R{\={u}}si{\c{n}}{\v{s}}},
	title={Best Increments for the Average Case of Shellsort},
	booktitle={Fundamentals of Computation Theory},
	date={2001-08-02},
	publisher={Springer Berlin Heidelberg},
	address={Berlin, Heidelberg},
	pages={106-117},
	abstract={This paper presents the results of using sequential analysis to find increment sequences that minimize the average running time of Shellsort, for array sizes up to several thousand elements. The obtained sequences outperform by about 3{\%} the best ones known so far, and there is a plausible evidence that they are the optimal ones."},
	isbn={978-3-540-44669-9},
	doi={10.1007/3-540-44669-9_12},
%	url={https://web.archive.org/web/20180923235211/http://sun.aei.polsl.pl/~mciura/publikacje/shellsort.pdf},
%	urldate={2024-05-24},
}

@misc{lee2021empirically,
	author={Ying Wai Lee},
	title={Empirically Improved Tokuda Gap Sequence in Shellsort},
	date={2021-12-21},
	eprint={2112.11112v1},
	archivePrefix={arXiv},
	primaryClass={cs.DS},
}

@report{lukas_geis,
	author={Lukas Geis},
	title={Random Number Generation in the Pim\hyphen{}Architecture},
	type={Research Project Report},
	institution={Goethe University Frankfurt},
	date={2024},
	version={8c11f1f},
	pagetotal={13},
	url={https://github.com/lukasgeis/upmem-rng/blob/main/report/report.pdf},
	urldate={2024-05-19},
}

@misc{axtmann2020engineering,
	author={Michael Axtmann and Sascha Witt and Daniel Ferizovic and Peter Sanders},
	title={Engineering In\hyphen{}place (Shared\hyphen{}memory) Sorting Algorithms},
	date={2021-02-03},
	eprint={2009.13569v2},
	archivePrefix={arXiv},
	primaryClass={cs.DC},
}

@article{wegener1993heapsort,
	author={Ingo Wegener},
	title={BOTTOM\hyphen{}UP\hyphen{}HEAPSORT, a new variant of HEAPSORT beating, on an average, QUICKSORT (if \(n\) is not very small)},
	journal={Theoretical Computer Science},
	volume={118},
	issue={1},
	pages={81-98},
	date={1993-09-13},
	issn={0304-3975},
	doi={10.1016/0304-3975(93)90364-Y},
%	doi={https://doi.org/10.1016/0304-3975(93)90364-Y},
%	url={https://www.sciencedirect.com/science/article/pii/030439759390364Y},
	abstract={A variant of HEAPSORT, called BOTTOM-UP-HEAPSORT, is presented. It is based on a new reheap procedure. This sequential sorting algorithm is easy to implement and beats, on an average, QUICKSORT if n⩾400 and a clever version of QUICKSORT (where the split object is the median of 3 randomly chosen objects) if n⩾16000. The worst-case number of comparisons is bounded by 1.5n log n+O(n). Moreover, the new reheap procedure improves the delete procedure for the heap data structure for all n.},
}

@article{hoare1962quicksort,
	author={Charles Antony Richard Hoare},
	title={Quicksort},
	journal={The Computer Journal},
	volume={5},
	number={1},
	pages={10-16},
	year={1962},
	month={01},
	abstract="{A description is given of a new method of sorting in the random-access store of a computer. The method compares very favourably with other known methods in speed, in economy of storage, and in ease of programming. Certain refinements of the method, which may be useful in the optimization of inner loops, are described in the second part of the paper.}",
	issn={0010-4620},
	doi={10.1093/comjnl/5.1.10},
%	url={https://doi.org/10.1093/comjnl/5.1.10},
%	eprint={https://academic.oup.com/comjnl/article-pdf/5/1/10/1111445/050010.pdf},
}

@article{erkiö1984worstcase,
	author={Hannu Erkiö},
	title={The Worst Case Permutation for Median\hyphen{}of\hyphen{}Three Quicksort},
	journal={The Computer Journal},
	volume={27},
	number={3},
	pages={276-277},
	year={1984},
	month={01},
	abstract="{An algorithm is given which forms the worst case permutation for one of the most efficient versions of quicksort (median-of-three quicksort). This makes the experimental evaluation of this important algorithm possible. The paper includes a simple experimental comparison of the median-of-three and original versions of quicksort.}",
	issn={0010-4620},
	doi={10.1093/comjnl/27.3.276},
%	url={https://doi.org/10.1093/comjnl/27.3.276},
%	eprint={https://academic.oup.com/comjnl/article-pdf/27/3/276/1049274/270276.pdf},
}

@online{blum2011probabilistic,
	author={Avrim Blum and Manuel Blum},
	title={Lecture 3},
	subtitle={Probabilistic Analysis and Randomized Quicksort},
	date={2011-09-06},
	url={https://www.cs.cmu.edu/~avrim/451f11/lectures/lect0906.pdf},
	organization={Carnegie Mellon University},
	urldate={2024-07-26},
}

@online{blum2011comparison,
	author={Avrim Blum and Manuel Blum},
	title={Lecture 5},
	subtitle={Comparison\hyphen{}based Lower Bounds for Sorting},
	date={2011-09-13},
	url={https://www.cs.cmu.edu/~avrim/451f11/lectures/lect0913.pdf},
	organization={Carnegie Mellon University},
	urldate={2024-07-26},
}

@article{blum1973median,
	author={Manuel Blum and Robert W. Floyd and Vaughan Pratt and Ronald L. Rivest and Robert E. Tarjan},
	title={Time bounds for selection},
	journal={Journal of Computer and System Sciences},
	volume={7},
	number={4},
	pages={448-461},
	date={1973-08},
	editor={Arnold Leonard Rosenberg},
	issn={0022-0000},
	doi={https://doi.org/10.1016/S0022-0000(73)80033-9},
%	url={https://www.sciencedirect.com/science/article/pii/S0022000073800339},
	abstract={The number of comparisons required to select the i-th smallest of n numbers is shown to be at most a linear function of n by analysis of a new selection algorithm—PICK. Specifically, no more than 5.4305 n comparisons are ever required. This bound is improved for extreme values of i, and a new lower bound on the requisite number of comparisons is also proved.},
}

@InProceedings{katajainen1997meticulous,
	author={Jyrki Katajainen and Jesper Larsson Tr{\"a}ff},
	editor={Giancarlo Bongiovanni and Daniel Pierre Bovet and Giuseppe Di Battista},
	title={A meticulous analysis of mergesort programs},
	booktitle={Algorithms and Complexity},
	date={1997-03-12},
	publisher={Springer Berlin Heidelberg},
	address={Berlin, Heidelberg},
	pages={217-228},
	abstract={The efficiency of mergesort programs is analysed under a simple unit-cost model. In our analysis the time performance of the sorting programs includes the costs of key comparisons, element moves and address calculations. The goal is to establish the best possible time-bound relative to the model when sorting n integers. By the well-known information-theoretic argument n log2n−O(n) is a lower bound for the integer-sorting problem in our framework. New implementations for two-way and four-way bottom-up mergesort are given, the worst-case complexities of which are shown to be bounded by 5.5n log2n+O(n) and 3.25n log2n+O(n), respectively. The theoretical findings are backed up with a series of experiments which show the practical relevance of our analysis when implementing library routines for internal-memory computations.},
	isbn={978-3-540-68323-0},
	doi={10.1007/3-540-62592-5_74},
}

@manual{upmemSDK,
	title={UPMEM DPU SDK Documentation},
	date={2024},
	version={2024.1.0},
	organization={UPMEM},
	location={Grenoble},
	url={https://sdk.upmem.com/2024.1.0},
}

@unpublished{upmem2021WhitePaper,
	author={UPMEM},
	title={UPMEM Processing In\hyphen{}Memory},
	date={2021-09},
	subtitle={Ultra-efficient acceleration for data\hyphen{}intensive applications},
	titleaddon={PIM Technology Paper},
	location={Grenoble},
}

@inproceedings{upmem2019HotChips,
	author={Fabrice Devaux},
	title={The true Processing In Memory accelerator},
	booktitle={Hot Chips 31 Symposium},
	date={2019-08-19},
	eventtitle={HC31},
	eventdate={2019-08-18/2019-08-20},
	venue={Stanford University},
	publisher={Institute of Electrical and Electronics Engineers},
	address={New York City},
	isbn={978-1-7281-2089-8},
	doi={10.1109/HOTCHIPS.2019.8875680},
}

@article{williams1964heapsort,
	author={John William Joseph Williams},
	title={Algorithm 232},
	subtitle={Heapsort},
	journaltitle={Communications of the ACM},
	date={1964-06-01},
	editor={George Elmer Forsythe},
	volume={7},
	issue={6},
	pages={347-348},
	doi={10.1145/512274.512284},
}

@article{floyd1964treesort,
	author={Robert W Floyd},
	title={Algorithm 245},
	subtitle={Treesort 3},
	journaltitle={Communications of the ACM},
	date={1964-12-01},
	editor={Calvin Carl Gotlieb},
	volume={7},
	issue={12},
	pages={701},
	doi={10.1145/355588.365103},
}

@article{codish2017sortingnetworks,
	author={Michael Codish and Luis Cruz\hyphen{}Filipe and Markus Nebel and Peter Schneider\hyphen{}Kamp},
	title={Optimizing Sorting Algorithms by Using Sorting Networks},
	journaltitle={Formal Aspects of Computing},
	date={2017-05-01},
	editor={Moreno Falaschi and Augusto Sampaio},
	volume={29},
	issue={3},
	pages={559-579},
	abstract={In this paper, we show how the theory of sorting networks can be applied to synthesize optimized general-purpose sorting libraries. Standard sorting libraries are often based on combinations of the classic Quicksort algorithm, with insertion sort applied as base case for small, fixed, numbers of inputs. Unrolling the code for the base case by ignoring loop conditions eliminates branching, resulting in code equivalent to a sorting network. By replacing it with faster sorting networks, we can improve the performance of these algorithms. We show that by considering the number of comparisons and swaps alone we are not able to predict any real advantage of this approach. However, significant speed-ups are obtained when taking advantage of instruction level parallelism and non-branching conditional assignment instructions, both of which are common in modern CPU architectures. Furthermore, a close control of how often registers have to be spilled to memory gives us a complete explanation of the performance of different sorting networks, allowing us to choose an optimal one for each particular architecture. Our experimental results show that using code synthesized from these efficient sorting networks as the base case for Quicksort libraries results in significant real-world speed-ups.},
	doi={10.1007/s00165-016-0401-3},
}

@online{m2015fastestway,
	author={{m69 ''snarky and unwelcoming'' [sic!].}},
	title={Fastest way to sort 10 numbers? (numbers are 32 bit)},
	date={2015-08-24},
	url={https://stackoverflow.com/a/32173153},
	organization={Stack Overflow},
	urldate={2024-08-11},
}

@online{paulr2010fastestsort,
	author={Paul R},
	title={Fastest sort of fixed length 6 int array},
	date={2010-05-07},
	url={https://stackoverflow.com/a/2786959},
	organization={Stack Overflow},
	urldate={2024-08-11},
}

@misc{peters2021patterndefeatingquicksort,
	title={Pattern\hyphen{}defeating Quicksort},
	author={Orson Raphael Lennard Peters},
	date={2021-06-09},
	eprint={2106.05123},
	archivePrefix={arXiv},
	primaryClass={cs.DS},
}

@InProceedings{wild2012averagecase,
	author={Sebastian Wild and Markus E. Nebel},
	editor={Leah Epstein and Paolo Ferragina},
	title={Average Case Analysis of Java 7's Dual Pivot Quicksort},
	booktitle={20th Annual European Symposium},
	maintitle={Algorithms},
%	date={2012-09-10},
	series={ESA '12},
	eventdate={2012-09-10/2012-09-12},
	location={Ljubljana},
	publisher={Springer Berlin Heidelberg},
	address={Berlin, Heidelberg},
	pages={825-836},
	abstract={Recently, a new Quicksort variant due to Yaroslavskiy was chosen as standard sorting method for Oracle's Java 7 runtime library. The decision for the change was based on empirical studies showing that on average, the new algorithm is faster than the formerly used classic Quicksort. Surprisingly, the improvement was achieved by using a dual pivot approach, an idea that was considered not promising by several theoretical studies in the past. In this paper, we identify the reason for this unexpected success. Moreover, we present the first precise average case analysis of the new algorithm showing e.g. that a random permutation of length n is sorted using {\$}1.9n{\backslash}ln n-2.46n+{\backslash}mathcal{\{}O{\}}({\backslash}ln n){\$}key comparisons and {\$}0.6n{\backslash}ln n+0.08n+{\backslash}mathcal{\{}O{\}}({\backslash}ln n){\$}swaps.},
	isbn={978-3-642-33090-2},
	doi={10.1007/978-3-642-33090-2_71},
}

@book{lang2009algorithmen,
	author={Hans Werner Lang},
	title={Algorithmen in Java},
	date={2009-12-16},
	editor={Margit Roth},
	language={German},
	edition={2},
	publisher={Oldenbourg Wissenschaftsverlag},
	location={München},
	isbn={978-3-486-59340-2},
	pagetotal={384},
	doi={10.1524/9783486593402},
}

@article{mutlu2022Benchmarking,
	author={Juan Gómez\hyphen{}Luna and Izzat El Hajj and Ivan Fernandez and Christina Giannoula and Geraldo Francisco de Oliveira and Onur Mutlu},
	journal={IEEE Access},
	title={Benchmarking a New Paradigm: Experimental Analysis and Characterization of a Real Processing\hyphen{}in\hyphen{}Memory System},
	date={2022-05-10},
	volume={10},
	pages={52565-52608},
	abstract={Many modern workloads, such as neural networks, databases, and graph processing, are fundamentally memory-bound. For such workloads, the data movement between main memory and CPU cores imposes a significant overhead in terms of both latency and energy. A major reason is that this communication happens through a narrow bus with high latency and limited bandwidth, and the low data reuse in memory-bound workloads is insufficient to amortize the cost of main memory access. Fundamentally addressing this data movement bottleneck requires a paradigm where the memory system assumes an active role in computing by integrating processing capabilities. This paradigm is known as processing-in-memory (PIM). Recent research explores different forms of PIM architectures, motivated by the emergence of new 3D-stacked memory technologies that integrate memory with a logic layer where processing elements can be easily placed. Past works evaluate these architectures in simulation or, at best, with simplified hardware prototypes. In contrast, the UPMEM company has designed and manufactured the first publicly-available real-world PIM architecture. The UPMEM PIM architecture combines traditional DRAM memory arrays with general-purpose in- order cores, called DRAM Processing Units (DPUs), integrated in the same chip. This paper provides the first comprehensive analysis of the first publicly-available real-world PIM architecture. We make two key contributions. First, we conduct an experimental characterization of the UPMEM-based PIM system using microbenchmarks to assess various architecture limits such as compute throughput and memory bandwidth, yielding new insights. Second, we present PrIM (Processing-In-Memory benchmarks), a benchmark suite of 16 workloads from different application domains (e.g., dense/sparse linear algebra, databases, data analytics, graph processing, neural networks, bioinformatics, image processing), which we identify as memory-bound. We evaluate the performance and scaling characteristics of PrIM benchmarks on the UPMEM PIM architecture, and compare their performance and energy consumption to their modern CPU and GPU counterparts. Our extensive evaluation conducted on two real UPMEM-based PIM systems with 640 and 2,556 DPUs provides new insights about suitability of different workloads to the PIM system, programming recommendations for software designers, and suggestions and hints for hardware and architecture designers of future PIM systems.},
	keywords={Computer architecture;Benchmark testing;Random access memory;Graphics processing units;Hardware;Software;Energy consumption;Processing-in-memory;near-data processing;memory systems;data movement bottleneck;DRAM;benchmarking;real-system characterization;workload characterization},
	doi={10.1109/ACCESS.2022.3174101},
	issn={2169-3536},
}

@article{giannoula2022sparse,
	author = {Giannoula, Christina and Fernandez, Ivan and G\'{o}mez-Luna, Juan and Koziris, Nectarios and Goumas, Georgios and Mutlu, Onur},
	title = {Towards Efficient Sparse Matrix Vector Multiplication on Real Processing-In-Memory Architectures},
	date = {2022-06-02},
	publisher = {Association for Computing Machinery},
	address = {New York},
	volume = {50},
	number = {1},
	issn = {0163-5999},
	doi = {10.1145/3547353.3522661},
	abstract = {Several manufacturers have already started to commercialize near-bank Processing-In-Memory (PIM) architectures, after decades of research efforts. Near-bank PIM architectures place simple cores close to DRAM banks. Recent research demonstrates that they can yield significant performance and energy improvements in parallel applications by alleviating data access costs. Real PIM systems can provide high levels of parallelism, large aggregate memory bandwidth and low memory access latency, thereby being a good fit to accelerate the Sparse Matrix Vector Multiplication (SpMV) kernel. SpMV has been characterized as one of the most significant and thoroughly studied scientific computation kernels. It is primarily a memory-bound kernel with intensive memory accesses due its algorithmic nature, the compressed matrix format used, and the sparsity patterns of the input matrices given. This paper provides the first comprehensive analysis of SpMV on a real-world PIM architecture, and presents SparseP, the first SpMV library for real PIM architectures. We make two key contributions. First, we design efficient SpMV algorithms to accelerate the SpMV kernel in current and future PIM systems, while covering a wide variety of sparse matrices with diverse sparsity patterns. Second, we provide the first comprehensive analysis of SpMV on a real PIM architecture. Specifically, we conduct our rigorous experimental analysis of SpMV kernels in the UPMEM PIM system, the first publicly-available real-world PIM architecture. Our extensive evaluation provides new insights and recommendations for software designers and hardware architects to efficiently accelerate the SpMV kernel on real PIM systems. For more information about our thorough characterization on the SpMV PIM execution, results, insights and the open-source SparseP software package [21], we refer the reader to the full version of the paper [3, 4]. The SparseP software package is publicly and freely available at https://github.com/CMU-SAFARI/SparseP.},
	journal = {SIGMETRICS Perform. Eval. Rev.},
	pages = {33–34},
	pagetotal = {337},
	keywords = {benchmarking, data movement bottleneck, dram, workload characterization, spmv library, sparse matrix-vector multiplication, real-system characterization, processing-in-memory, near-data processing, multicore, memory systems, high-performance computing}
}

@book{hennessy2012computer,
	author = {John Leroy Hennessy and David Andrew Patterson},
	title = {Computer Architecture},
	date = {2011-09-29},
	foreword = {Luiz André Barroso},
	subtitle = {A Quantitative Approach},
	edition = {5},
	publisher = {Morgan Kaufmann Publishers Inc.},
	location = {San Francisco},
	isbn = {978-0-12-383872-8},
	pagetotal = {880},
	abstract = {The computing world today is in the middle of a revolution: mobile clients and cloud computing have emerged as the dominant paradigms driving programming and hardware innovation today. The Fifth Edition of Computer Architecture focuses on this dramatic shift, exploring the ways in which software and technology in the "cloud" are accessed by cell phones, tablets, laptops, and other mobile computing devices. Each chapter includes two real-world examples, one mobile and one datacenter, to illustrate this revolutionary change. Updated to cover the mobile computing revolutionEmphasizes the two most important topics in architecture today: memory hierarchy and parallelism in all its forms.Develops common themes throughout each chapter: power, performance, cost, dependability, protection, programming models, and emerging trends ("What's Next")Includes three review appendices in the printed text. Additional reference appendices are available online.Includes updated Case Studies and completely new exercises.},
}

@misc{mutlu23memorycentric,
	title={Memory\hyphen{}Centric Computing},
	author={Onur Mutlu},
	date={2023-09-13},
	eprint={2305.20000},
	archivePrefix={arXiv},
	primaryClass={cs.AR},
}

@inproceedings{boroumand2018google,
	author = {Boroumand, Amirali and Ghose, Saugata and Kim, Youngsok and Ausavarungnirun, Rachata and Shiu, Eric and Thakur, Rahul and Kim, Daehyun and Kuusela, Aki and Knies, Allan and Ranganathan, Parthasarathy and Mutlu, Onur},
	editor={Matthew Fluet},
	title = {Google Workloads for Consumer Devices},
	subtitle = {Mitigating Data Movement Bottlenecks},
	date = {2018-03-19},
	eventdate = {2018-03-24/2018-03-28},
	isbn = {978-1-4503-4911-6},
	publisher = {Association for Computing Machinery},
	address = {New York},
	doi = {10.1145/3173162.3173177},
	abstract = {We are experiencing an explosive growth in the number of consumer devices, including smartphones, tablets, web-based computers such as Chromebooks, and wearable devices. For this class of devices, energy efficiency is a first-class concern due to the limited battery capacity and thermal power budget. We find that data movement is a major contributor to the total system energy and execution time in consumer devices. The energy and performance costs of moving data between the memory system and the compute units are significantly higher than the costs of computation. As a result, addressing data movement is crucial for consumer devices. In this work, we comprehensively analyze the energy and performance impact of data movement for several widely-used Google consumer workloads: (1) the Chrome web browser; (2) TensorFlow Mobile, Google's machine learning framework; (3) video playback, and (4) video capture, both of which are used in many video services such as YouTube and Google Hangouts. We find that processing-in-memory (PIM) can significantly reduce data movement for all of these workloads, by performing part of the computation close to memory. Each workload contains simple primitives and functions that contribute to a significant amount of the overall data movement. We investigate whether these primitives and functions are feasible to implement using PIM, given the limited area and power constraints of consumer devices. Our analysis shows that offloading these primitives to PIM logic, consisting of either simple cores or specialized accelerators, eliminates a large amount of data movement, and significantly reduces total system energy (by an average of 55.4\% across the workloads) and execution time (by an average of 54.2\%).},
	booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
	pages = {316–331},
	pagetotal = {811},
	keywords = {consumer workloads, data movement, energy efficiency, memory systems, processing-in-memory},
	location = {Williamsburg},
	series = {ASPLOS '18}
}

@InProceedings{kestor2013quantifying,
	author={Gokcen Kestor and Roberto Gioiosa and Darren James Kerbyson and Adolfy Hoisie},
	booktitle={International Symposium on Workload Characterization},
	title={Quantifying the energy cost of data movement in scientific applications},
	date={2014-01-09},
	series={IISWC '13},
	eventdate={2013-09-22/2013-09-24},
	location={Portland},
	publisher={Institute of Electrical and Electronics Engineers},
	address={New York City},
	pages={56-65},
	abstract={In the exascale era, the energy cost of moving data across the memory hierarchy is expected to be two orders of magnitude higher than the cost of performing a double-precision floating point operation. Despite its importance, the energy cost of data movement in scientific applications has not be quantitatively evaluated even for current systems.},
	keywords={Benchmark testing;Energy measurement;Prefetching;Energy consumption;Registers;Power demand;Fans},
	doi={10.1109/IISWC.2013.6704670},
	isbn={978-1-4799-0555-3},
}

@article{peccerillo2022survey,
	title = {A survey on hardware accelerators},
	subtitle = {Taxonomy, trends, challenges, and perspectives},
	journal = {Journal of Systems Architecture},
	volume = {129},
	pages = {102561},
	date = {2022-05-24},
	issn = {1383-7621},
	doi = {10.1016/j.sysarc.2022.102561},
	author = {Biagio Peccerillo and Mirco Mannino and Andrea Mondelli and Sandro Bartolini},
	keywords = {Accelerators, Domain-Specific Architectures, Survey, Taxonomy, Classification, Data-parallel, Machine Learning, PIM, CGRA, Open challenges, Future research directions},
	abstract = {In recent years, the limits of the multicore approach emerged in the so-called “dark silicon” issue and diminishing returns of an ever-increasing core count. Hardware manufacturers, out of necessity, switched their focus to accelerators, a new paradigm that pursues specialization and heterogeneity over generality and homogeneity. They are special-purpose hardware structures separated from the CPU with aspects that exhibit a high degree of variability. We define a taxonomy based on fourteen of these aspects, grouped in four macro-categories: general aspects, host coupling, architecture, and software aspects. According to it, we categorize around 100 accelerators of the last decade from both industry and academia, and critically analyze emerging trends. We complete our discussion with throughput and efficiency figures. Then, we discuss some prominent open challenges that accelerators are facing, analyzing state-of-the-art solutions, and suggesting prospective research directions for the future.}
}

@article{li2017analogue,
	title={Analogue signal and image processing with large memristor crossbars},
	author={Can Li and Miao Hu and Miao Hu and Yunning Li and Hao Jiang and Ning Ge and Eric Montgomery and Jiaming Zhang and Wenhao Song and Noraica D{\'a}vila and Catherine E. Graves and Zhiyong Li and John Paul Strachan and Peng Lin and Zhongrui Wang and Mark D. Barnell and Qing Wu and R. Stanley Williams and J. Joshua Yang and Qiangfei Xia},
	journal={Nature Electronics},
	date={2017-12-04},
	volume={1},
	pages={52-59},
	doi={10.1038/s41928-017-0002-z},
	issn={2520-1131},
}

@InProceedings{liu2020recnmp,
	author={Ke, Liu and Gupta, Udit and Cho, Benjamin Youngjae and Brooks, David and Chandra, Vikas and Diril, Utku and Firoozshahian, Amin and Hazelwood, Kim and Jia, Bill and Lee, Hsien-Hsin S. and Li, Meng and Maher, Bert and Mudigere, Dheevatsa and Naumov, Maxim and Schatz, Martin and Smelyanskiy, Mikhail and Wang, Xiaodong and Reagen, Brandon and Wu, Carole\hyphen{}Jean and Hempstead, Mark and Zhang, Xuan},
	editor={Lisa O'Conner},
	booktitle={47th Annual International Symposium on Computer Architecture},
	title={RecNMP: Accelerating Personalized Recommendation with Near\hyphen{}Memory Processing},
	date={2020-07-13},
	series={ISCA '20},
	eventdate={2020-05-30/2020-06-03},
	location={Valencia},
	publisher={Institute of Electrical and Electronics Engineers},
	address={New York City},
	pages={790-803},
	keywords={Systematics;Limiting;Scheduling algorithms;Energy conservation;Random access memory;Production;Parallel processing},
	doi={10.1109/ISCA45697.2020.00070}
}

@InProceedings{gagandeep2020nero,
	author={Singh, Gagandeep and Diamantopoulos, Dionysios and Hagleitner, Christoph and Gomez\hyphen{}Luna, Juan and Stuijk, Sander and Mutlu, Onur and Corporaal, Henk},
	booktitle={30th International Conference on Field-Programmable Logic and Applications},
	title={NERO: A Near High\hyphen{}Bandwidth Memory Stencil Accelerator for Weather Prediction Modeling},
	date={2020-10-13},
	series={FPL '20},
	eventdate={2020-08-31/2020-09-04},
	location={Gothenburg},
	publisher={Institute of Electrical and Electronics Engineers},
	address={New York City},
	pages={9-17},
	abstract={Ongoing climate change calls for fast and accurate weather and climate modeling. However, when solving large-scale weather prediction simulations, state-of-the-art CPU and GPU implementations suffer from limited performance and high energy consumption. These implementations are dominated by complex irregular memory access patterns and low arithmetic intensity that pose fundamental challenges to acceleration. To overcome these challenges, we propose and evaluate the use of near-memory acceleration using a reconfigurable fabric with high-bandwidth memory (HBM). We focus on compound stencils that are fundamental kernels in weather prediction models. By using high-level synthesis techniques, we develop NERO, an FPGA+HBM-based accelerator connected through IBM CAPI2 (Coherent Accelerator Processor Interface) to an IBM POWER9 host system. Our experimental results show that NERO outperforms a 16-core POWER9 system by 4.2x and 8.3x when running two different compound stencil kernels. NERO reduces the energy consumption by 22x and 29x for the same two kernels over the POWER9 system with an energy efficiency of 1.5 GFLOPS/Watt and 17.3 GFLOPS/Watt. We conclude that employing near-memory acceleration solutions for weather prediction modeling is promising as a means to achieve both high performance and high energy efficiency.},
	keywords={Energy consumption;Multithreading;Weather forecasting;Predictive models;Energy efficiency;Climate change},
	doi={10.1109/FPL50879.2020.00014},
	issn={1946-1488},
	isbn={978-1-7281-9902-3},
}

@InProceedings{hyun2024pathfinding,
	author={Hyun, Bongjoon and Kim, Taehun and Lee, Dongjae and Rhu, Minsoo},
	booktitle={International Symposium on High-Performance Computer Architecture},
	title={Pathfinding Future PIM Architectures by Demystifying a Commercial PIM Technology},
	date={2024-04-02},
	series={HPCA '24},
	eventdate={2024-03-02/2024-03-06},
	location={Edinburgh},
	publisher={Institute of Electrical and Electronics Engineers},
	address={New York City},
	pages={263-279},
	abstract={Processing-in-memory (PIM) has been explored for decades by computer architects, yet it has never seen the light of day in real-world products due to its high design overheads and lack of a killer application. With the advent of critical memoryintensive workloads, several commercial PIM technologies have been introduced to the market, ranging from domain-specific PIM architectures to more general-purpose PIM architectures. In this work, we deepdive into UPMEM's commercial PIM technology, a general-purpose PIM-enabled parallel computing architecture that is highly programmable. Our first key contribution is the development of a flexible simulation framework for PIM. The simulator we developed (aka uPIMulator) enables the compilation of UPMEM-PIM source codes into its compiled machine-level instructions, which are subsequently consumed by our cycle-level performance simulator. Using uPIMulator, we demystify UPMEM's PIM design through a detailed characterization study. Finally, we identify some key limitations of the current UPMEM-PIM system through our case studies and present some important architectural features that will become critical for future PIM architectures to support.},
	keywords={Microarchitecture;Source coding;Computer architecture;Parallel processing;Vectors;Distance measurement;Processing-In-Memory (PIM);Near-Memory Processing;Parallel Architecture},
	doi={10.1109/HPCA57654.2024.00029},
	issn={2378-203X},
}

@InProceedings{gogineni2024swiftrl,
	author={Gogineni, Kailash and Dayapule, Sai Santosh and Gómez-Luna, Juan and Gogineni, Karthikeya and Wei, Peng and Lan, Tian and Sadrosadati, Mohammad and Mutlu, Onur and Venkataramani, Guru},
	booktitle={International Symposium on Performance Analysis of Systems and Software},
	title={SwiftRL},
	subtitle={Towards Efficient Reinforcement Learning on Real Processing-In-Memory Systems},
	date={2024-06-16},
	series={ISPASS '24},
	eventdate={2024-05-05/2024-05-07},
	location={Indianapolis},
	publisher={Institute of Electrical and Electronics Engineers},
	address={New York City},
	pages={217-229},
	keywords={Training;Performance evaluation;Silver;Q-learning;Software algorithms;Graphics processing units;Computer architecture;Reinforcement learning;Processing-in-memory;Multi-agent sys-tems;Memory bottleneck;Performance analysis},
	doi={10.1109/ISPASS61541.2024.00029},
	isbn={979-8-3503-7638-8},
}

@InProceedings{nider2021casestudy,
	author = {Joel Nider and Craig Mustard and Andrada Zoltan and John Ramsden and Larry Liu and Jacob Grossbard and Mohammad Dashti and Romaric Jodin and Alexandre Ghiti and Jordi Chauzi and Alexandra Fedorova},
	title = {A Case Study of Processing\hyphen{}in\hyphen{}Memory in off\hyphen{}the\hyphen{}Shelf Systems},
	booktitle = {USENIX Annual Technical Conference},
	date = {2021-07},
	eventdate={2021-07-14/2021-07-16},
	series={USENIX ATC '21},
	isbn = {978-1-939133-23-6},
	pages = {117-130},
	url = {https://www.usenix.org/conference/atc21/presentation/nider},
	publisher = {USENIX Association},
}

@misc{chen2024updlrm,
	title={UpDLRM},
	subtitle={Accelerating Personalized Recommendation using Real\hyphen{}World PIM Architecture},
	author={Sitian Chen and Haobin Tan and Amelie Chi Zhou and Yusen Li and Pavan Balaji},
	date={2024-06-20},
	eprint={2406.13941},
	archivePrefix={arXiv},
	primaryClass={cs.IR},
}

@article{musser1999introspective,
	author = {David R. Musser},
	title = {Introspective Sorting and Selection Algorithms},
	journaltitle = {Software: Practice and Experience},
	date = {1999-01-08},
	volume = {27},
	issue = {8},
	pages = {983-993},
	doi = {10.1002/(SICI)1097-024X(199708)27%3A8<983%3A%3AAID-SPE117>3.0.CO%3B2-%23},
}

@online{peters2002timsort,
	author = {Tim Peters},
	title = {Timsort},
	date = {2002},
	url = {https://bugs.python.org/file4451/timsort.txt},
	urldate = {2024-10-10},
}

@online{skarupke2016ska,
	author = {Malte Skarupke},
	title = {I Wrote a Faster Sorting Algorithm},
	date = {2016-12-27},
	url = {https://probablydance.com/2016/12/27/i-wrote-a-faster-sorting-algorithm/},
	urldate = {2024-10-10},
}

@article{bingmann2015engineering,
	author = {Timo Bingmann and Andreas Eberle and Peter Sanders},
	title = {Engineering Parallel String Sorting},
	journaltitle = {Algorithmica},
	date = {2015-09-18},
	editor = {Mohammad Taghi Hajiaghayi},
	volume = {77},
	number = {2},
	pages = {235-285},
	issn = {1432-0541},
	doi = {10.1007/s00453-015-0071-1},
}

@InProceedings{kaligosi2009misprediction,
	author={Kanela Kaligosi and Peter Sanders},
	editor={Yossi Azar and Thomas Erlebach},
	title={How Branch Mispredictions Affect Quicksort},
	booktitle={14th Annual European Symposium},
	maintitle={Algorithms},
%	date={2012-09-10},
	series={ESA '06},
	eventdate={2012-09-11/2012-09-13},
	location={Zurich},
	publisher={Springer Berlin Heidelberg},
	address={Berlin, Heidelberg},
	pages={780-791},
	abstract={We explain the counterintuitive observation that finding “good” pivots (close to the median of the array to be partitioned) may not improve performance of quicksort. Indeed, an intentionally skewed pivot improves performance. The reason is that while the instruction count decreases with the quality of the pivot, the likelihood that the direction of a branch is mispredicted also goes up. We analyze the effect of simple branch prediction schemes and measure the effects on real hardware.},
	isbn={978-3-540-38875-3},
	doi={10.1007/11841036},
}

@misc{edelkamp2016misprediction,
	title={BlockQuicksort: How Branch Mispredictions don't affect Quicksort},
	author={Stefan Edelkamp and Armin Weiß},
	year={2016},
	eprint={1604.06697},
	archivePrefix={arXiv},
	primaryClass={cs.DS},
	eprint={1604.06697v2},
}

@book{cormen2013algorithmen,
	author={Thomas H. Cormen and Charles Eric Leiserson and Ronald Linn Rivest and Clifford Seth Stein},
	title={Algorithmen},
	date={2013-10-17},
	translator={Paul Molitor},
	subtitle={Eine Einführung},
	language={German},
	origlanguage={English},
	edition={4},
	publisher={Oldenbourg Wissenschaftsverlag},
	location={Munich},
	isbn={978-3-486-74861-1},
	pagetotal={1319},
}

@book{maurer1974datenstrukturen,
	author={Hermann Maurer and Hans\hyphen{}Werner Six},
	title={Datenstrukturen und Programmierverfahren},
	date={1974},
	editor={Heinrich Görtler},
	language={German},
	series={Leitfäden der angewandten Mathematik und Mechanik},
	number={25},
	note={Teubner Studienbücher Informatik},
	publisher={B. G. Teubner},
	location={Stuttgart},
	isbn={3-519-02328-8},
	pagetotal={222},
}

@book{wirth1975algorithmen,
	author={Niklaus Wirth},
	title={Algorithmen und Datenstrukturen},
	date={1975},
	editor={Heinrich Görtler},
	language={German},
	series={Leitfäden der angewandten Mathematik und Mechanik},
	number={32},
	note={Teubner Studienbücher Informatik},
	publisher={B. G. Teubner},
	location={Stuttgart},
	isbn={3-519-02330-X},
	pagetotal={376},
}
