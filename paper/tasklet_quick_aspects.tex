\subsection{Presentation of Key Aspects}
\label{sec:tasklet:quick:aspects}

\paragraph{Sentinel Values}
\label{sec:tasklet:quick:imp:sentinels}
In order to dispose of many bounds checks on the pointers, the partitioning presented above does not exactly follow \citeauthor{hoare1962quicksort}'s original scheme, where pointers stopped only if \lstinline|*i > *p| and \lstinline|*j < *p|.
Instead, by stopping if \lstinline|*i >= *p| and \lstinline|*j <= *p|, the pivot \lstinline|p| acts as sentinel value for both pointers as the stopping condition is met there definitely.
This means that they cannot leave the array during their very first motion onwards.

If one pointer surpasses the pivot before the other reaches it, they act as additional sentinel values for each other.
If the left pointer \lstinline|i| reaches the right pointer \lstinline|j|, then all elements behind pointer \lstinline|j| are already in the correct partition, that is, they are at least as great as the pivot.
An analogous argument can be made for when pointer \lstinline|j| meets pointer \lstinline|i|.
Of course, they can also stop directly on each other if the value on which they stopped is equal to the pivot.
In consequence, only one bounds check is needed during partitioning, namely whether \lstinline|j <= i| holds whenever both pointers stopped.

A downside to this modification is that elements equal to the pivot are also swapped during partitioning.
But even on inputs with many duplicates, this is a price worth paying, as experiments show.


\paragraph{Pivot Positioning}
By a further modification, one can find the final position of the pivot, so it need not be touched anymore in the future.
After the pivot \lstinline|p| is selected, it is swapped with the last element of the array.
Consequently, the right pointer \lstinline|j| has to begin at the second to last element.
Since the right-hand partition contains only elements at least as great as the pivot, the pivot must be the minimum of that partition.
Therefore, once the partitioning is over, the last element of the array, that is the pivot, can be swapped with the first element of the right-hand partition, that is the element with address \lstinline|i|.
The right-hand partition can be shortened to begin at address \lstinline|i + 1| instead of \lstinline|i|.


\paragraph{Base Cases}
When only a few elements remain in a partition, \QS{}'s overhead predominates such that \IS{} lends itself as fallback algorithm.
A speedup of up to \num{1.67} is achieved by falling back.
As shown in \cref{fig:quick:fallback}, the optimal threshold for switching the sorting algorithm is 18 elements for 32-bit integers on uniformly distributed inputs.
For 64-bit integers, the optimal threshold is 17 elements.
Notwithstanding, we set 18 elements to be the default threshold for both data types to simplify matters since the impact is minuscule.
For sorted and almost sorted inputs, the threshold is higher since \IS{} performs well on them, so falling back earlier and, thus, ending the sorting process is better.
Because \QS{}'s two pointers invert large portions of reverse sorted inputs while partitioning, the same holds true for them, too, even though they represent \IS{}'s worst case.
However, such input distributions should be catered for by a pattern-defeating \QS{} as laid out in \cref{sec:tasklet:conclusion}.

To avoid unnecessary uses of \IS{}, another base case is imaginable, namely terminating when a partition contains at most one element.
There are tremendous consequences for the runtime depending on the exact implementation of the base cases, as shown later in \cref{sec:tasklet:quick:compilation}.

\begin{figure}[t]
	\pgfplotstableset{
		create on use/n/.style={create col/copy column from table={data/quick/fallback/uint32/16.txt}{n}},
	}
	\pgfplotsinvokeforeach{14,15,16,17,18,19,20}{
		\pgfplotstableset{create on use/µ_#1_32/.style={create col/copy column from table={data/quick/fallback/uint32/#1.txt}{µ_TrivialBC}}}
		\pgfplotstableset{create on use/µ_#1_64/.style={create col/copy column from table={data/quick/fallback/uint64/#1.txt}{µ_TrivialBC}}}
	}
	\pgfplotstablenew[columns={n,µ_14_32,µ_15_32,µ_16_32,µ_17_32,µ_18_32,µ_19_32,µ_20_32,µ_14_64,µ_15_64,µ_16_64,µ_17_64,µ_18_64,µ_19_64,µ_20_64}]{\pgfplotstablegetrowsof{data/quick/fallback/uint32/16.txt}}{\tableQuickFallback}

	\tikzsetnextfilename{quick_fallback}
	\begin{tikzpicture}[plot]
		\begin{groupplot}[
			horizontal sep for labels,
			adaptive group=1 by 2,
			groupplot ylabel={Speedup},
			x from 16 to 1024,
			ymin=0.994,
			ymax=1.002,
			yticklabel style={/pgf/number format/.cd, precision=3, fixed, zerofill},
		]
			\nextgroupplot[title/.add={}{32-bit}]
			\pgfplotsset{legend to name=leg:quick:fallback, legend entries={\(16\),\(17\),\(19\),\(20\)}}
			\pgfplotsinvokeforeach{16,17,19,20}{
				\plotspeedup{#1_32}{18_32}{tableQuickFallback}
			}
			%
			\nextgroupplot[title/.add={}{64-bit}]
			\pgfplotsinvokeforeach{16,17,19,20}{
				\plotspeedup{#1_64}{18_64}{tableQuickFallback}
			}
		\end{groupplot}
	\end{tikzpicture}

	\tikzexternaldisable\hfil\pgfplotslegendfromname{leg:quick:fallback}\hfil\tikzexternalenable
	\caption{
		Speedups of \QS*{} with different thresholds (16--20) for when to fall back to \IS{} over a \QS{} with a threshold of 18 elements, on uniformly distributed integers.
		Using \ShS{} is not beneficial because many partitions undercut the thresholds significantly.
	}
	\label{fig:quick:fallback}
\end{figure}


\paragraph{Recursion vs.\ Iteration}
In theory, the question of whether an algorithm should be implemented recursively or iteratively comes down to convenience.
Due to the uniform costs of instructions, jumping to the beginning of a loop or to the beginning of a function costs the same, as does managing arguments automatically through the regular call stack or manually through a simulated one.
Furthermore, in case of \QS{}, the compiler turns tail-recursive%
\footnote{
	A recursive call is \emph{tail-recursive} if it is the final operation performed by the callee.
} calls into jumps back to the beginning of the function, so that one partition is sorted recursively and the other iteratively.
All this would suggest a recursive \QS{} due to its simpler implementation.

In practice, it comes down to the compilation.
Even parts of the algorithms which are independent from the choice between recursion and iteration can be compiled differently, such that there are implementations where being iterative is better than being recursive and the other way around.
%Overall though, iterative implementations tend to be compiled better with superior register usage and fewer instructions used for the actual \QS{} algorithm.
A detailed analysis is given in \cref{sec:tasklet:quick:compilation}.


\paragraph{Partition Prioritisation}
Always sorting the shorter partition first and putting the longer partition on the call stack guarantees that the problem size is at least halved each step, so that the call stack stores \(\bigoh{\log n}\) elements at most.
Unfortunately, this approach proves to be detrimental to the quality of the compilation, as shown in \cref{sec:tasklet:quick:compilation}.
Instead, it is advisable to always prioritise the same side.
Whether the left-hand or the right-hand partition is sorted first should not make any difference for the runtime but even this choice changes the quality of the compilation;
in this thesis, the right-hand partitions are prioritised.


\paragraph{Pivot Selection}
Another parameter to tune is the way in which the pivot is selected.
The following methods were implemented and benchmarked:
\begin{itemize}
	\item
	Using the \emph{last element} is the fastest method, requiring zero additional instructions.

	\item
	Taking the \emph{deterministic median} of three elements, namely the first, middle, and last one, is more computationally expensive since the position of the middle element must be calculated, the median be determined, and the pivot be swapped with the last element of the array.

	\item
	A \emph{random element} is most efficiently drawn on a \ac{DPU} when using an xorshift random number generator and rejection sampling \cite{lukas_geis}.

	\item
	The \emph{random median} is a combination of the previous two methods, where the median of three random elements is taken.
	For simplicity, there is no check on whether an element is drawn twice or thrice.
	The chances of this happening are low, though, since the partitions are rather long.
\end{itemize}
Taking a median increases the probability of selecting a pivot that is neither particularly little nor particularly great.
This leads to more balanced partitions so that the call stack is less likely to overflow and the base cases are reached faster.
But as long as one of the deterministic methods is used, it is possible to construct inputs where the runtime climbs up to \(\bigoh*{n^2}\)~\cite{erkiö1984worstcase}.
Such a runtime could occur, for example, when the pivot is always the minimum or maximum of the partition so that everything is moved to the same partition.
As a consequence, the problem size would be reduced by only one element (namely the pivot) after each partitioning step.
This problematic behaviour is aggravated by the call stack overflowing easily because of the static partition prioritisation.

The random pivots circumvent this issue.
Whilst the pivots could, by ill luck, also lead to the same unbalanced partitions as the deterministic pivots, the worst-case expected runtime is in \(\bigoh{n \log n}\) \cite{blum2011probabilistic}.
The median of medians \cite{blum1973median} guarantees a runtime of \(\bigoh{n \log n}\) but was not implemented because a performant implementation would presumably be quite complex and its benefit for this thesis minuscule.
