\subsection{Evaluation of the Performance}
\label{sec:mram:merge:performance}

%%% Table with different values for CACHE_SIZE and SEQREAD_CACHE_SIZE.
\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergeFS/.style={}, create on use/σ_MergeFS/.style={}, columns={n,µ_MergeFS,σ_MergeFS}]{0}{\tableMergeMramCxxviii}
\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergeFS/.style={}, create on use/σ_MergeFS/.style={}, columns={n,µ_MergeFS,σ_MergeFS}]{0}{\tableMergeMramCclvi}
\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergeFS/.style={}, create on use/σ_MergeFS/.style={}, columns={n,µ_MergeFS,σ_MergeFS}]{0}{\tableMergeMramDxii}
\pgfplotsinvokeforeach{11,12,16}{
	\pgfplotstablevertcat{\tableMergeMramCxxviii}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=256/SEQREAD_CACHE_SIZE=128/uint32/uniform.txt}
	\pgfplotstablevertcat{\tableMergeMramCxxviii}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=512/SEQREAD_CACHE_SIZE=128/uint32/uniform.txt}
	\pgfplotstablevertcat{\tableMergeMramCxxviii}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=1024/SEQREAD_CACHE_SIZE=128/uint32/uniform.txt}

	\pgfplotstablevertcat{\tableMergeMramCclvi}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=256/SEQREAD_CACHE_SIZE=256/uint32/uniform.txt}
	\pgfplotstablevertcat{\tableMergeMramCclvi}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=512/SEQREAD_CACHE_SIZE=256/uint32/uniform.txt}
	\pgfplotstablevertcat{\tableMergeMramCclvi}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=1024/SEQREAD_CACHE_SIZE=256/uint32/uniform.txt}

	\pgfplotstablevertcat{\tableMergeMramDxii}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=256/SEQREAD_CACHE_SIZE=512/uint32/uniform.txt}
	\pgfplotstablevertcat{\tableMergeMramDxii}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=512/SEQREAD_CACHE_SIZE=512/uint32/uniform.txt}
	\pgfplotstablevertcat{\tableMergeMramDxii}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=1024/SEQREAD_CACHE_SIZE=512/uint32/uniform.txt}
}

\pgfplotstablenew[
	create on use/tasklets/.style={create col/set list={11,,,12,,,16,,}},
	create on use/cache size/.style={create col/set list={256,512,1024,256,512,1024,256,512,1024}},
	create on use/seqread 128/.style={create col/copy column from table={\tableMergeMramCxxviii}{µ_MergeFS}},
	create on use/seqread 256/.style={create col/copy column from table={\tableMergeMramCclvi}{µ_MergeFS}},
	create on use/seqread 512/.style={create col/copy column from table={\tableMergeMramDxii}{µ_MergeFS}},
	columns={tasklets,cache size,seqread 128,seqread 256,seqread 512},
]{9}{\tableMergeMramXxxii}

\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergeFS/.style={}, create on use/σ_MergeFS/.style={}, columns={n,µ_MergeFS,σ_MergeFS}]{0}{\tableMergeMramLxivCxxviii}
\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergeFS/.style={}, create on use/σ_MergeFS/.style={}, columns={n,µ_MergeFS,σ_MergeFS}]{0}{\tableMergeMramLxivCclvi}
\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergeFS/.style={}, create on use/σ_MergeFS/.style={}, columns={n,µ_MergeFS,σ_MergeFS}]{0}{\tableMergeMramLxivDxii}
\pgfplotsinvokeforeach{11,12,16}{
	\pgfplotstablevertcat{\tableMergeMramLxivCxxviii}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=256/SEQREAD_CACHE_SIZE=128/uint64/uniform.txt}
	\pgfplotstablevertcat{\tableMergeMramLxivCxxviii}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=512/SEQREAD_CACHE_SIZE=128/uint64/uniform.txt}
	\pgfplotstablevertcat{\tableMergeMramLxivCxxviii}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=1024/SEQREAD_CACHE_SIZE=128/uint64/uniform.txt}

	\pgfplotstablevertcat{\tableMergeMramLxivCclvi}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=256/SEQREAD_CACHE_SIZE=256/uint64/uniform.txt}
	\pgfplotstablevertcat{\tableMergeMramLxivCclvi}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=512/SEQREAD_CACHE_SIZE=256/uint64/uniform.txt}
	\pgfplotstablevertcat{\tableMergeMramLxivCclvi}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=1024/SEQREAD_CACHE_SIZE=256/uint64/uniform.txt}

	\pgfplotstablevertcat{\tableMergeMramLxivDxii}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=256/SEQREAD_CACHE_SIZE=512/uint64/uniform.txt}
	\pgfplotstablevertcat{\tableMergeMramLxivDxii}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=512/SEQREAD_CACHE_SIZE=512/uint64/uniform.txt}
	\pgfplotstablevertcat{\tableMergeMramLxivDxii}{data/merge_mram/NR_TASKLETS=#1/CACHE_SIZE=1024/SEQREAD_CACHE_SIZE=512/uint64/uniform.txt}
}

\pgfplotstablenew[
	create on use/tasklets/.style={create col/set list={11,,,12,,,16,,}},
	create on use/cache size/.style={create col/set list={256,512,1024,256,512,1024,256,512,1024}},
	create on use/seqread 128/.style={create col/copy column from table={\tableMergeMramLxivCxxviii}{µ_MergeFS}},
	create on use/seqread 256/.style={create col/copy column from table={\tableMergeMramLxivCclvi}{µ_MergeFS}},
	create on use/seqread 512/.style={create col/copy column from table={\tableMergeMramLxivDxii}{µ_MergeFS}},
	columns={tasklets,cache size,seqread 128,seqread 256,seqread 512},
]{9}{\tableMergeMramLxiv}



%%% Comparison of normalised runtimes.
\pgfkeys{/pgf/fpu,/pgf/fpu/output format=fixed}
\pgfplotstablegetelem{8}{seqread 128}\of{\tableMergeMramXxxii}
\pgfmathsetmacro{\timeMergeMramSeqreadSmall}{\pgfplotsretval}
\pgfplotstablegetelem{8}{seqread 256}\of{\tableMergeMramXxxii}
\pgfmathsetmacro{\timeMergeMramSeqreadLarge}{\pgfplotsretval}
\pgfmathsetmacro{\ratioSeqreadSmallLarge}{ (\timeMergeMramSeqreadLarge / \timeMergeMramSeqreadSmall - 1) * 100 }

\pgfplotstablegetelem{2}{seqread 512}\of{\tableMergeMramXxxii}
\pgfmathsetmacro{\timeMergeMramEleven}{\pgfplotsretval}
\pgfmathsetmacro{\timeMergeMramElevenNorm}{ \timeMergeMramEleven / ((2^23 / 11) * log2((2^23 / 11))) }
\pgfplotstablegetelem{5}{seqread 512}\of{\tableMergeMramXxxii}
\pgfmathsetmacro{\timeMergeMramTwelve}{\pgfplotsretval}
\pgfmathsetmacro{\timeMergeMramTwelveNorm}{ \timeMergeMramTwelve / ((2^23 / 12) * log2((2^23 / 12))) }
\pgfmathsetmacro{\timeMergeMramTwelveNormExpected}{ \timeMergeMramElevenNorm * 12 / 11 }
\pgfplotstablegetelem{8}{seqread 512}\of{\tableMergeMramXxxii}
\pgfmathsetmacro{\timeMergeMramSixteen}{\pgfplotsretval}
\pgfmathsetmacro{\timeMergeMramSixteenNorm}{ \timeMergeMramSixteen / ((2^23 / 16) * log2((2^23 / 16))) }
\pgfmathsetmacro{\timeMergeMramSixteenNormExpected}{ \timeMergeMramElevenNorm * 16 / 11 }
\pgfkeys{/pgf/fpu=false}



%%% Bar plot.
\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergeHS/.style={}, create on use/σ_MergeHS/.style={}, columns={n,µ_MergeHS,σ_MergeHS}]{0}{\tableMergeMramHalf}
\expandafter\pgfplotsinvokeforeach\expandafter{\alldists}{
	\pgfplotstablevertcat{\tableMergeMramHalf}{data/merge_mram/half-space/STABLE=false/uint32/#1.txt}
}
\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergeFS/.style={}, create on use/σ_MergeFS/.style={}, columns={n,µ_MergeFS,σ_MergeFS}]{0}{\tableMergeMramFull}
\expandafter\pgfplotsinvokeforeach\expandafter{\alldists}{
	\pgfplotstablevertcat{\tableMergeMramFull}{data/merge_mram/NR_TASKLETS=16/CACHE_SIZE=1024/SEQREAD_CACHE_SIZE=512/uint32/#1.txt}
}

\pgfplotstablenew[
	create on use/dist/.style={create col/set list={Sorted,Reverse S.,Almost S.,Zero-One,Uniform,Zipf's}},
	create on use/half/.style={create col/copy column from table={\tableMergeMramHalf}{µ_MergeHS}},
	create on use/full/.style={create col/copy column from table={\tableMergeMramFull}{µ_MergeFS}},
	columns={dist,half,full}
]{6}{\tableMergeMramHalfFullXxxii}

\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergeHS/.style={}, create on use/σ_MergeHS/.style={}, columns={n,µ_MergeHS,σ_MergeHS}]{0}{\tableMergeMramHalf}
\expandafter\pgfplotsinvokeforeach\expandafter{\alldists}{
	\pgfplotstablevertcat{\tableMergeMramHalf}{data/merge_mram/half-space/STABLE=false/uint64/#1.txt}
}
\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergeFS/.style={}, create on use/σ_MergeFS/.style={}, columns={n,µ_MergeFS,σ_MergeFS}]{0}{\tableMergeMramFull}
\expandafter\pgfplotsinvokeforeach\expandafter{\alldists}{
	\pgfplotstablevertcat{\tableMergeMramFull}{data/merge_mram/NR_TASKLETS=16/CACHE_SIZE=1024/SEQREAD_CACHE_SIZE=512/uint64/#1.txt}
}

\pgfplotstablenew[
	create on use/dist/.style={create col/set list={Sorted,Reverse S.,Almost S.,Zero-One,Uniform,Zipf's}},
	create on use/half/.style={create col/copy column from table={\tableMergeMramHalf}{µ_MergeHS}},
	create on use/full/.style={create col/copy column from table={\tableMergeMramFull}{µ_MergeFS}},
	columns={dist,half,full}
]{6}{\tableMergeMramHalfFullLxiv}



With only a single tasklet computing, the optimal parameter choice is clearly to set \cachesize{} and \seqreadcachesize{} to the maximum value in order to minimise \acp{DMA}.
Employing eleven tasklets is profitable despite limiting the possible values for the parameters severely, as the pipeline is utilised fully then, allowing to execute more instructions at the same time.
Comparing the runtimes of a sole tasklet performing an \ac{MRAM} \MS{} and of a tasklet with ten other ones working concurrently shows an increase of the runtime below the ten percent mark.
This tiny increase is owed to multiple tasklets performing \acp{DMA} at the same time, stalling all involved tasklets but one since data transfers happen sequentially.
Employing twelve tasklets not only makes simultaneous \acp{DMA} more frequent, it also increases the effective execution time of a single instruction from eleven cycles to twelve cycles.
The effect is dampened by the latency hiding, as tasklets performing a \ac{DMA} are suspended.
Of course, using more than eleven tasklets may also be more convenient from an algorithmic point of view.
%This is the case for the parallel \MS{} presented in \cref{sec:par}, where sixteen tasklets are easier to handle than twelve and twelve easier than eleven.
For this reason, we will look at the influence of different values for \cachesize{} and \seqreadcachesize{} with eleven, twelve and sixteen tasklets in this \lcnamecref{sec:mram:merge:performance}.
Please note that \unrollfactor{} is always set to 8 and cannot be set greater, for the kernel would, otherwise, not fit within the \ac{IRAM} anymore.
Also, \unrolledcachelength{} is always set to \cachesize{} / \lstinline[keywords={}]|sizeof(T)|.

\begin{table}[t]
	\centering
	\sisetup{
		table-alignment-mode=format,
		table-format=4.0,
		round-precision=0,
		round-mode=places,
		exponent-mode=fixed,
		fixed-exponent=6,
		text-font-command=\addfontfeatures{RawFeature=+tnum}
	}
	\pgfplotstableset{
		every head row/.style={
			before row=\toprule Tasklets & \lstinline|CACHE| & \multicolumn{6}{c}{4 × \lstinline|SEQREAD_CACHE|} \\ \cmidrule{3-5},
			after row=\midrule,
		},
		every last row/.style={after row=\bottomrule},
		every nth row={3}{before row=\addlinespace},
		column type={c c S S S},
		columns/tasklets/.style={column name={}, string type},
		columns/cache size/.style={column name={}, string type},
		seqread/.style={column name={#1}, string type},
		columns/seqread 128/.style={seqread={512}},
		columns/seqread 256/.style={seqread={1024}},
		columns/seqread 512/.style={seqread={2048}},
		multicolumn names,
	}
	\begin{subtable}{0.49\textwidth}
		\centering
		\pgfplotstabletypeset{\tableMergeMramXxxii}
		\caption{
			32-bit integers (\(n = 2^{23} / \mathit{Tasklets}\))
		}
	\end{subtable}
	\hfil
	\begin{subtable}{0.49\textwidth}
		\centering
		\pgfplotstabletypeset{\tableMergeMramLxiv}
		\caption{
			64-bit integers (\(n = 2^{22} / \mathit{Tasklets}\))
		}
	\end{subtable}

	\caption{
		Runtimes of the sequential \ac{MRAM} \MS{} in hundred thousand cycles for different values of \cachesize{} and \seqreadcachesize{} on \qty[exponent-mode=input]{32}{\mebi\byte} of uniformly distributed inputs.
		The input is divided evenly amongst the eleven to sixteen tasklets, which sort their proportion of the input concurrently.
		Measurements represents the means of the maximum runtimes, that is the wall-clock times.
	}
	\label{tab:mram:sizes}
\end{table}

Since the main purpose of the sequential \ac{MRAM} \MS{} is being a component of the parallel \ac{MRAM} \MS{}, measurements are restricted to the maximum amount of data which the \ac{MRAM} can hold.
The parallel \MS{} is essentially a full-space \MS{}, needing auxiliary space of the same size as the input.
As the total size of the \ac{MRAM} is \qty{64}{\mebi\byte}, the total input size is capped at \qty{32}{\mebi\byte}.
Although the logging buffer of a \ac{DPU} resides in the \ac{MRAM}, this buffer is not created if no use of \lstinline|printf|, \lstinline|puts|, and \lstinline|putchar| is detected in the program.
Therefore, the entirety of the \ac{MRAM} is indeed freely usable.

\Cref{tab:mram:sizes} shows the runtime of the sequential \MS{} for different values of \cachesize{} and \seqreadcachesize{}.
The number of tasklets is eleven, twelve, and thirteen, and each tasklet is assigned the same proportion of the input, which is \liningnums{2\textsuperscript{23}} and \liningnums{2\textsuperscript{22}} elements for 32-bit and 64-bit integers, respectively.
Whilst the measurements were conducted on a uniform input distribution, we confirmed that the general observations hold true for the other input distributions as well.
We opt to show the mean of the maximum runtimes of individual tasklets, that is the \emph{wall-clock times}, since tasklets have to wait for each other in the parallel \MS{}.
The difference between maximum and average runtimes is barely noticeably, however, being in the magnitude of one per mille for deterministic input distributions and in the magnitude of one per myriad for the random ones.

In most cases, it holds that the larger caches and buffers are, the faster the execution is.
This suggests that being possibly stalled for longer is outweighed by the fewer occurrences of \ac{DMA} overheads, especially when many tasklets are present.
Also, the larger one of the parameters is, the greater the gain from enlarging the other one tends to be.
The speedup has a tendency to be greater for 64-bit integers than for 32-bit integers.
Still, some exceptions to these observations appear.
For example, with sixteen tasklets, \cachesize{} = 1024, and 32-bit integers, doubling the combined size of two sequential-read buffers from \qty{512}{\byte} to \qty{1024}{\byte} increases the runtime by around \qty[round-mode=places, round-precision=0]{\ratioSeqreadSmallLarge}{\percent}.
In summary, though, it is reasonable to set all parameters to the largest values allowed for by the limited \ac{WRAM}, that is \cachesize{} = 1024 and \seqreadcachesize{} = 512.

As final remark to \cref{tab:mram:sizes}, it should be noted that the reduction of the runtime through more tasklets is solely owed to the log-linear runtime of \MS{} and individual tasklets having to sort fewer elements.
Indeed, the effect of the overfull pipeline becomes apparent either when computing the normalised runtimes of \cref{tab:mram:sizes} or, simpler, when looking at \cref{tab:mram:sizes_same_n} where the number of elements sorted by each tasklet is kept constant.
%With 32-bit integers, it is \qty[round-mode=places, round-precision=1]{\timeMergeMramElevenNorm}{\cycle} for eleven tasklets, \qty[round-mode=places, round-precision=1]{\timeMergeMramTwelveNorm}{\cycle} for twelve, and \qty[round-mode=places, round-precision=1]{\timeMergeMramSixteenNorm}{\cycle} for sixteen.
%However, these normalised runtimes still evince that suspended tasklets free the pipeline up, as multiplying the normalised runtime for eleven tasklets by \Lfrac{12/11} and \Lfrac{16/11} yields \qty[round-mode=places, round-precision=1]{\timeMergeMramTwelveNormExpected}{\cycle} and \qty[round-mode=places, round-precision=1]{\timeMergeMramSixteenNormExpected}{\cycle}, which would be expected if no \acp{DMA} were involved.

\begin{figure}
	\tikzsetnextfilename{merge_mram}
	\begin{tikzpicture}[plot]
		\begin{groupplot}[
			adaptive group=1 by 2,
			barplot,
		]
			\nextgroupplot[title/.add={}{32-bit}]
			\pgfplotsset{legend to name=leg:merge:mram, legend entries={Full-Space, Half-Space}}
			\addplot+ table[x=dist, y=full] {\tableMergeMramHalfFullXxxii};
			\addplot+ table[x=dist, y=half] {\tableMergeMramHalfFullXxxii};
			%
			\nextgroupplot[title/.add={}{64-bit}]
			\addplot+ table[x=dist, y=full] {\tableMergeMramHalfFullLxiv};
			\addplot+ table[x=dist, y=half] {\tableMergeMramHalfFullLxiv};
		\end{groupplot}
	\end{tikzpicture}

	\vspace{-4pt}
	\tikzexternaldisable\hfil\pgfplotslegendfromname{leg:merge:mram}\hfil\tikzexternalenable
	\caption{
		Mean of the wall-clock times of concurrently executed \ac{MRAM} \MS*{} on all benchmarked input distributions and data types.
		Sixteen tasklets were employed, each sorting \liningnums{2\textsuperscript{19}} many 32-bit integers and \liningnums{2\textsuperscript{18}} many 64-bit integers.
		Starting runs are formed using \QS{}.
	}
	\label{fig:merge:mram}
\end{figure}

Next, we look at the performance of the sequential \MS{}, both in a half-space and a full-space variant, across all benchmarked input distributions.
Even though the parallel sorting algorithm presented in \cref{sec:par} is a type of \MS{}, its performance is improved when forgoing stability.
Thence, it makes sense to analyse the performance of the sequential \ac{MRAM} \MS{} with both \QS{} and \MS{} as \ac{WRAM} sorting algorithms used during the formation of the starting runs.
With \QS{}, \cref{fig:merge:mram} shows that like its \ac{WRAM} counterpart, the full-space \ac{MRAM} \MS{} works the fastest on sorted inputs, which is because one of the runs in each pair of runs is flushed in its entirety without overhead from comparisons and elementwise moves.
Reverse sorted inputs are sorted slower despite the same flushing pattern.
This is explicable through the worse performance of \QS{} \Dash or, rather, \IS{} \Dash on reverse sorted inputs.
The same phenomenon occurs with \ac{WRAM} \MS*{}, but there, the influence of \IS{} on the total runtime is much higher because of the shorter inputs, leading to this input distribution being their worst case.
Almost sorted inputs are also sorted slower, however, this is because more elements need to be compared before non-depleted runs can be flushed:
When a pair of runs is merged, one of them contains mainly little elements, whereas the other one contains mainly great elements.
If the former run contains just one great element, it will be merged only almost entirely at first.
Upon reaching the one greater element, some elements of the latter run are merged before the last element of the other run can finally be merged and a cheap flush be performed.
Late flushes become more frequent with uniform and Zipf's input distributions, explaining why they former constitutes the worst case.
The ranking of the input distributions stays about the same when using \MS{} instead of \QS{} to form the starting runs with a notable exception.
For the \ac{WRAM} \MS{}, reverse sorted inputs constitute the worst case by such a wide margin that they become the worst case for the \ac{MRAM} \MS{}, too, as shown in \cref{fig:merge:mram_unstable}.

For the half-space \MS{}, the picture is different.
Only on sorted inputs does the half-space \MS{} manage to beat its full-space counterpart, and it falls behind on all other input distributions decisively.
This is because copying the first runs has become too costly due to the \acp{DMA}, and not having to flush the second runs yields a considerable advantage only on sorted inputs.
In conclusion, merging \ac{WRAM} data is the fastest with the half-space variant, while merging \ac{MRAM} data is the fastest with the full-space variant.
