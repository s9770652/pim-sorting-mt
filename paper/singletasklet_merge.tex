\subsection{\texorpdfstring{\MS{}}{MergeSort}}
\label{subsec:tasklet:merge}

\pgfplotstablereadnamed{data/wram_sorts.txt}{tableWramSorts}
\def\mergealgos{16,24,32,48,64,96}
\expandafter\pgfplotsinvokeforeach\expandafter{\mergealgos}{
	\pgfplotstablereadnamed{data/merge/fallback=#1/uint32/sorted.txt}{tableMergeStart#1_32sorted}
	\pgfplotstablereadnamed{data/merge/fallback=#1/uint32/reverse.txt}{tableMergeStart#1_32reverse}
	\pgfplotstablereadnamed{data/merge/fallback=#1/uint32/almost.txt}{tableMergeStart#1_32almost}
	\pgfplotstablereadnamed{data/merge/fallback=#1/uint32/uniform.txt}{tableMergeStart#1_32uniform}
	\pgfplotstablereadnamed{data/merge/fallback=#1/uint32/zipf.txt}{tableMergeStart#1_32zipf}
	\pgfplotstablereadnamed{data/merge/fallback=#1/uint32/normal.txt}{tableMergeStart#1_32normal}
}

\MS{} repeatedly compares two sorted subarrays and merges them into a bigger sorted array in time \(\bigtheta{n \log n}\).
Unlike \QS{}, this runtime is guaranteed.
Furthermore, the sorting is naturally stable.

\paragraph{Starting Runs}
Instead of starting by merging runs of length 1, it is beneficial to first create bigger starting runs using \ShS{}.
Unlike \QS{}, where each partition naturally acted as sentinel for the subsequent one, it is necessary to temporarily place sentinels values in front of each starting run and later restore the original values of the preceding run.
The step sizes used for \ShS{} \Dash namely \(\stepsizes = (1)\) for lengths up to 16, \(\stepsizes = (6, 1)\) for lengths up to 48, and \(\stepsizes = (12, 5, 1)\) for everything above \Dash have been chosen based on the results in \cref{subsec:tasklet:shell}, according to which these step sizes offer top performance for uniformly distributed inputs and medial performance for the reverse sorted inputs.
Spot-check inspection suggest no deterioration of \ShS{}'s compilation due to inlining.

\paragraph{Memory Footprint}
A simple but fast implementation of \MS{} writes all merged runs to an auxiliary array, raising the need for space for \(n\) additional elements (\enquote{full space}).
After a round is finished and all pairs of runs have been merged, the input array and the auxiliary array switch roles, and the merging starts anew.
Are the final sorted elements supposed to be saved in the original input array, a final round with a write-back from the auxiliary array to the input array is needed for some input lengths.

A slightly more sophisticated implementation needs space for only \(\sfrac{n}{2}\) additional elements (\enquote{half space}):
When two adjacent runs are to be merged, the first one can be copied to an auxiliary array.
Then, the copy and the second run are merged to the start of the first run.
As a side effect, no write-back is ever needed and, additionally, the merging of two runs can be terminated prematurely once the last element of the copied run is merged, since the last elements of the other run are already in place.
%As a consequence, flushes will only be performed on at most half of the runs.
Further optimised, \MS{} would not need to copy the first runs immediately.
It suffices to search for the foremost element of the first run which is bigger than the first element of the second element.
All previous elements are already in the correct position so only the following elements need to be copied to the auxiliary array.
This optimisation, although examined during development, was not in use when measuring runtimes since it unfortunately complicates another optimisation, namely unrolling.

\paragraph{Unrolling}
There are four common reasons for \emph{flushing}, that is, writing \Dash many oftwhiles \Dash consecutive elements:
\begin{enumerate}
	\item
	When two runs are merged and the end of one of them is reached, the remaining elements of the other one can be moved safely to the output location.
	Especially with the sorted, reverse sorted, and almost sorted input distributions, the number of remaining elements will be high.

	\item
	The number of runs is odd, so the full-space \MS{} moves the last run to the output location immediately.

	\item
	The full-space \MS{} may write all elements from the auxiliary array back to the input array if the former contains the final sorted sequence.

	\item
	The half-space \MS{} copies runs, whose length are always a multiple of the the starting run length, before each merger of pairs.
\end{enumerate}
Therefore, flushing account for a considerable part of the runtime, and reducing the loop overhead (variable incrementation and bounds checking) is helpful.
This can be done via \emph{unrolling}:
As long as at least, let us say, \(x\) elements still need to be flushed, the \(x\) foremost elements are moved first and then all necessary variables are incremented by \(x\).
Is \(x\) a compile-time constant, the compiler implements the moving of the elements through \(x\) instruction which use constant, pre-calculated offsets.
Once less than \(x\) elements remain, an ordinary loop which moves elements individually is used.
In good cases, this approach reduces the loop overhead to an \(x\)-th, whilst in bad cases, where less than \(x\) are to be flushed, the overhead is increased by one additional check.

Due to time reasons, we refrained from doing automatic and extensive tests and relied on manual and exploratory tests to come up with the following strategy:
When the full-space \MS{} performs a write-back or when the half-space \MS{} copies the first run, \(x\) is set to the starting run length.
In all other cases, \(x\) is set to 24.
This strategy, albeit not optimal, makes the \MS*{} significantly faster:
Sorting sorted, reverse sorted, and almost sorted inputs sees speed-ups up to 30\%, whereas sorting more random inputs still sees speed-ups for the most part and slow-downs into low single-digits at worst, depending on the starting run length.



\subsubsection*{Investigating the Compilation}

Yet again, the compiler shows unforeseen behaviour.
The following is a collocation of some of the issues found while engineering \MS{}.
They will not be discussed in detail here but still provide a point of reference for future work:
\begin{itemize}
	\item
	As already mentioned, sorting the starting runs via \ShS{} requires the placement and later removal of temporary sentinel values.
	For the very first starting run, one can omit storing and restoring the overwritten elements by using permanent sentinel values;
	this optimisation was in use when measuring runtimes.
	On the downside, this leads to a bigger compilation as \ShS{} is inlined twice.
	If the size of the whole compilation is already close to the maximum, one might be inclined to handle the first starting run just like the others.
	Realistically, this should slow down the total runtime by just a few hundreds of cycles, yet the real slow-down is in the thousands.

	\item
	If the input is so short that it fits entirely within the first starting run, one can immediately end the execution after \ShS{} is done.
	Several implementations were tested, with unsatisfactory results:
	Some increased the runtime for longer inputs, others decreased it but also increased it for shorter inputs.
	The settled-on implementation is of the former variety since the increases hit shorter inputs harder relatively and a more thorough solution would not further the purpose of this section.

	\item
	Concerning the half-space \MS{}:
	Treating the copied run logically as the second run and the uncopied run as the first one nets a noticeable decrease in runtime compared to an implementation with flipped logic.
	Even worse, only with the former does unrolling improve the speed, being an impairment with the latter!
	This behaviour occurs with both immediate and deferred copying of the first runs.
	An inspection of the issue unearthed marvels like code of the form
	\begin{center}
		\vspace{-\baselineskip}
		\texttt{*i++ = *j; a = b - i; c = i; i = d;}
	\end{center}
	leading to 5\% longer runtimes compared to
	\begin{center}
		\texttt{*i = *j; a = b - (i + 1); c = i + 1; i = d;}
	\end{center}
	even though executed at most once per merger, but we could sadly not pinpoint the fundamental cause for the behaviour.
\end{itemize}



\subsubsection*{Evaluation of the Performance}

\pgfplotsset{
	merge fallback/.style={
		horizontal sep for ticks,
		adaptive group=1 by 3,
		groupplot xlabel={Input Length \(n\)},
		groupplot ylabel={Cycles / \((n \lb n)\)},
		xmode=log,
		xmax=1024,
		xtick={16, 64, 256, 1024},
		xticklabels={\(16\), \(64\), \(256\), \(1024\)},
		minor xtick={32, 128, 512},
		enlarge x limits=true,
		legend columns=-1,
	},
	merge sort filter 16/.style={x filter/.expression={(\thisrow{n} == 16) || (\thisrow{n} ==  24) || (\thisrow{n} ==  96) || (\thisrow{n} == 384) || (\thisrow{n} == 1536) ? \pgfmathresult : nan}, /tikz/mark indices={1,2,3,4}},
	merge sort filter 24/.style={x filter/.expression={(\thisrow{n} == 16) || (\thisrow{n} ==  32) || (\thisrow{n} == 128) || (\thisrow{n} == 512) || (\thisrow{n} == 2048) ? \pgfmathresult : nan}},
	merge sort filter 32/.style={x filter/.expression={(\thisrow{n} == 16) || (\thisrow{n} ==  48) || (\thisrow{n} == 192) || (\thisrow{n} == 768) || (\thisrow{n} == 3072) ? \pgfmathresult : nan}},
	merge sort filter 48/.style={x filter/.expression={(\thisrow{n} == 16) || (\thisrow{n} ==  64) || (\thisrow{n} == 256) || (\thisrow{n} == 1024) ? \pgfmathresult : nan}},
	merge sort filter 64/.style={x filter/.expression={(\thisrow{n} == 16) || (\thisrow{n} ==  96) || (\thisrow{n} == 384) || (\thisrow{n} == 1536) ? \pgfmathresult : nan}, /tikz/mark indices={1,2,3}},
	merge sort filter 96/.style={x filter/.expression={(\thisrow{n} == 16) || (\thisrow{n} == 128) || (\thisrow{n} == 512) || (\thisrow{n} == 2048) ? \pgfmathresult : nan}},
}

\begin{figure}
	\tikzsetnextfilename{merge_starting_runs}
	\begin{tikzpicture}[plot]
		\begin{groupplot}[
			merge fallback,
			ymin=65,
			ymax=90,
			ytick distance=5,
		]
			\nextgroupplot[title={No Write-back\strut}, legend to name=leg:merge:starting_runs]
			\expandafter\legend\expandafter{\mergealgos}
			\clip (0, 0) rectangle (1024, 200);
			\expandafter\pgfplotsinvokeforeach\expandafter{\mergealgos}{
				\plotpernlogn[merge sort filter #1]{Merge}{tableMergeStart#1_32uniform}
			}
			%
			\nextgroupplot[title={Write-back\strut}]
			\clip (0, 0) rectangle (1024, 200);
			\expandafter\pgfplotsinvokeforeach\expandafter{\mergealgos}{
				\plotpernlogn[merge sort filter #1]{MergeWriteBack}{tableMergeStart#1_32uniform}
			}
			%
			\nextgroupplot[title={Half Space}]
			\clip (0, 0) rectangle (1024, 200);
			\expandafter\pgfplotsinvokeforeach\expandafter{\mergealgos}{
				\plotpernlogn[merge sort filter #1]{MergeHalfSpace}{tableMergeStart#1_32uniform}
			}
		\end{groupplot}
	\end{tikzpicture}

	\hfil\pgfplotslegendfromname{leg:merge:starting_runs}\hfil
	\caption{
		Comparison of \MS*{}, which need an auxiliary array of length either \(n\) (\enquote{No Write-back} / \enquote{Write-back}) or \(\sfrac{n}{2}\) (\enquote{Half Space}), for different lengths of the starting runs.
		The \MS*{} use a \ShS{} with the step sizes \(\stepsizes = (1)\) for length 16, \(\stepsizes = (6, 1)\) for lengths 24 to 48, and \(\stepsizes = (12, 5, 1)\) for lengths 64 and 96, respectively.
	}
	\label{fig:merge:starting_runs}
\end{figure}

Three implementations have been tested:
full space \MS{} without write-backs, full space \MS{} with write-backs, and half space \MS{}.
\Cref{fig:merge:starting_runs,fig:merge:starting_runs_uint32sorted,fig:merge:starting_runs_uint32uniform,fig:merge:starting_runs_uint64sorted,fig:merge:starting_runs_uint64uniform} show their performance for various starting run lengths.
Please note that the plots are smoothed:
Whenever the number of rounds increments, the runtimes hike, making the zigzagging plots cross each other unswervingly and, thereby, hard to read.
Thence, the figures contain marks for select measurements only in such a way that the resulting plots act as an upper bound on the runtime.

The measurements show that the \MS*{} guarantee a runtime of \(\bigoh{n \lb n}\) as expected.
The differences in runtime between the different input distributions are small compared to \QS{} and are ascribable to \ShS{} and to the differing suitability of the unrolling;
cases where the usage of \ShS{} worsened the runtime are unbeknown.

Even though the tested starting run lengths range from 16 to 96 elements, the mean runtime differences are surprisingly small.
Notwithstanding that the optimal choice depends on the specific input length because of the zigzagging, a starting run length of 32 elements fares decidedly well on average across all tested scenarios.

The half-space \MS{} delivers a strong performance despite its vastly lower memory footprint.
With 32-bit integers, it beats the full-space \MS{} without write-backs by 11\% on sorted inputs and effectively ties on all other inputs but the reverse sorted ones where it narrowly falls behind.
Naturally, the full-space \MS{} with write-backs is consistently (with the exception of reverse sorted inputs) at a disadvantage, despite seeing some light with inferior starting run lengths.
With 64-bit integers, the full-space \MS{} without write-backs manages to turn the ties into scant leads in the range from 1\% to 3\%.
Using the \MS{} with write-backs is still unprofitable.

In summary, a proper implementation of half-space \MS{} with deferred copying and fine-tuned unrolling would require some work but has the potential to be the overall best stable sorting algorithm.

\begin{figure}
	\tikzsetnextfilename{wram_sorts}
	\begin{tikzpicture}[plot]
		\begin{groupplot}[
			horizontal sep for labels,
			adaptive group=1 by 2,
			groupplot xlabel={Input Length \(n\)},
			xmode=log,
			xtick={20, 32, 64, 128, 256, 512, 1024},
			xticklabels={\(20\), \(32\), \(64\), \(128\), \(256\), \(512\), \(1024\)},
			legend columns=3,
		]
			\nextgroupplot[ylabel=Cycles / \((n \lb n)\), ymin=0, ymax=200, legend to name=leg:wram_sorts]
			\legend{\MS{} (no write-back), \QS{}, \ShS, \MS{} (write-back), \HS{}, \MS{} (half space)}
			\plotpernlogn{Merge}{tableWramSorts}
			\plotpernlogn{Quick}{tableWramSorts}
			\plotpernlogn{Shell}{tableWramSorts}
			\plotpernlogn{MergeWriteBack}{tableWramSorts}
			\plotpernlogn{Heap}{tableWramSorts}
			\plotpernlogn{MergeHalfSpace}{tableWramSorts}
			%
			\nextgroupplot[ylabel=Speed-up, ymin=0.3, ymax=1, extra y ticks={0.3}]
			\plotspeedup{Merge}{Quick}{tableWramSorts}
			\pgfplotsset{cycle list shift=1}
			\plotspeedup{Shell}{Quick}{tableWramSorts}
			\plotspeedup{MergeWriteBack}{Quick}{tableWramSorts}
			\plotspeedup{Heap}{Quick}{tableWramSorts}
			\plotspeedup{MergeHalfSpace}{Quick}{tableWramSorts}
		\end{groupplot}
	\end{tikzpicture}

	\hfil\pgfplotslegendfromname{leg:wram_sorts}\hfil
	\caption{
		Comparison of \MS{}, \HS{}, \ShS{}, and \QS{}.
		Due to \MS{}'s increased space requirements, its runtime was measured only for up to 768 elements.
		The \ShS{} uses the step sizes from \cref{fig:shell:against_others}, which are unoptimised for large input sizes.
		The speed-ups are with respect to the \QS{}.
	}
	\label{fig:wram_sorts}
\end{figure}
