\section{\texorpdfstring{\QS{}}{QuickSort}}
\label{sec:tasklet:quick}

\QS{} \cites{hoare1962quicksort}[88-91]{maurer1974datenstrukturen}[Chapter~2.2.6]{wirth1975algorithmen} uses partitioning to sort in an expected average runtime of \(\bigoh{n \log n}\) and a worst-case runtime of \(\bigoh{n^2}\):
A so-called \emph{pivot} element is selected from the input array according to some method, then the whole input array gets scanned and elements greater or less than the pivot are moved to the right-hand or left-hand side of the array, that is the \emph{partitions}, respectively;
elements equal to the pivot are allowed to be in either partition.
Finally, \QS{} is used on the left-hand and right-hand partition.

The partitioning is implemented using a modification of \citeauthor{hoare1962quicksort}'s original scheme \cite{hoare1962quicksort}:
After the pivot \lstinline|p| is selected, two pointers are set to either end of the array.
The left pointer \lstinline|i| moves rightwards until finding an element at least as great as the pivot (\lstinline|*i >= *p|).
Then, the right pointer \lstinline|j| moves leftwards until finding an element at most as great as the pivot (\lstinline|*j <= *p|).
Are the two elements found unequal, they are in the wrong order, so swapping them puts them in the right partitions.
Are they equal, swapping them anyway does not violate the order.
After swapping the elements, the pointers move onwards as described.
This process of repeated swaps continues until the pointers pass each other.
Where the right pointer \lstinline|j| came to rest marks the end of the left-hand partition, and where the left pointer \lstinline|i| did marks the beginning of the right-hand partition.

\QS{} does not sort in-place, as additional space of size \(\bigoh{\log n}\) is needed for a call stack.
Furthermore, \QS{} is not stable.


\paragraph{Sentinel Values}
\label{sec:tasklet:quick:imp:sentinels}
So as to avoid manifold bounds checks on the pointers, the partitioning presented above does not exactly follow \citeauthor{hoare1962quicksort}'s original scheme, where pointers halted only if \lstinline|*i > *p| and \lstinline|*j < *p|.
Instead, by halting if \lstinline|*i >= *p| and \lstinline|*j <= *p|, the pivot \lstinline|p| acts as sentinel value for both pointers as the halting condition is met there definitely.
This means that they cannot leave the array during their very first onwards movement.

If one pointer surpasses the pivot before the other reaches it, they act as additional sentinel values for each other.
If the left pointer \lstinline|i| reaches the right pointer \lstinline|j|, then all elements behind pointer \lstinline|j| are already in the correct partition, that is at least as great as the pivot.
An analogous argument can be made for when pointer \lstinline|j| meets pointer \lstinline|i|.
Of course, they can also halt directly on each other if the value they halted on is equal to the pivot.
In consequence, only one check is needed during partitioning, namely whether it holds \lstinline|j <= i| whenever both pointers halted.

A downside to this modification is that elements equal to the pivot are also swapped during partitioning.
But even on an input following Zipf's law, where many duplicates exist, this is a price worth paying.


\paragraph{Pivot Positioning}
By a further modification, one can find the final position of the pivot, so it needs not be touched anymore in the future.
After the pivot \lstinline|p| is selected, it is swapped with the last element of the array.
Moreover, the right pointer \lstinline|j| begins at the second-last element.
Since the right-hand partition contains only elements at least as great as the pivot, the pivot must be the minimum of that partition.
Therefore, once the partitioning is over, the last element of the array, that is the pivot, can be swapped with the first element of the right-hand partition, that is the element at address \lstinline|i|.
The right-hand partition now begins at address \lstinline|i + 1| instead of \lstinline|i|.


\paragraph{Base Cases}
When only a few elements remain in a partition, \QS{}'s overhead predominates such that \IS{} lends itself as fallback algorithm.
Up to 40\% of the runtime is saved by falling back.
As seen in \cref{fig:quick:fallback}, the optimal threshold for switching the sorting algorithm is 18 elements for 32-bit integers on uniform inputs and likely similar on inputs following Zipf's or normal distributions.
For 64-bit integers, the optimal threshold is 17 elements.
Notwithstanding, we set 18 elements to be the default threshold for both data types to simplify matters since the impact is minuscule.
For sorted and almost sorted inputs, the threshold is higher since \IS{} performs well on them, so falling back earlier and, thus, ending the sorting process is better.
Because \QS{}'s two pointers invert large swaths of reverse sorted inputs while partitioning, the same is true for that input distribution even though it is \IS{}'s worst-case.
However, such input distributions should be catered for by a pattern-defeating \QS{} as laid out in \cref{sec:tasklet:conclusion}, hence the 18 elements as default threshold altogether.

To avoid unnecessary uses of \IS{}, another base case is imaginable, namely terminating when a partition contains at most one element.
There are tremendous consequences for the runtime depending on the exact implementation of the base cases, as shown later in \enquote{\nameref{sec:tasklet:quick:compilation}}.
\begin{figure}[t]
	\pgfplotstableset{
		create on use/n/.style={create col/copy column from table={data/quick/fallback/uint32/16.txt}{n}},
	}
	\pgfplotsinvokeforeach{14,15,16,17,18,19,20}{
		\pgfplotstableset{create on use/µ_#1_32/.style={create col/copy column from table={data/quick/fallback/uint32/#1.txt}{µ_TrivialBC}}}
		\pgfplotstableset{create on use/µ_#1_64/.style={create col/copy column from table={data/quick/fallback/uint64/#1.txt}{µ_TrivialBC}}}
	}
	\pgfplotstablenew[columns={n,µ_14_32,µ_15_32,µ_16_32,µ_17_32,µ_18_32,µ_19_32,µ_20_32,µ_14_64,µ_15_64,µ_16_64,µ_17_64,µ_18_64,µ_19_64,µ_20_64}]{\pgfplotstablegetrowsof{data/quick/fallback/uint32/16.txt}}{\tableQuickFallback}

	\tikzsetnextfilename{quick_fallback}
	\begin{tikzpicture}[plot]
		\begin{groupplot}[
			horizontal sep for labels,
			adaptive group=1 by 2,
			groupplot ylabel={Speed-up},
			x from 16 to 1024,
			ymin=0.994,
			ymax=1.002,
			yticklabel style={/pgf/number format/.cd, precision=3, fixed, zerofill},
		]
			\nextgroupplot[title/.add={}{32-bit}]
			\pgfplotsset{legend to name=leg:quick:fallback, legend entries={16,17,19,20}}
			\pgfplotsinvokeforeach{16,17,19,20}{
				\plotspeedup{#1_32}{18_32}{tableQuickFallback}
			}
			%
			\nextgroupplot[title/.add={}{64-bit}]
			\pgfplotsinvokeforeach{16,17,19,20}{
				\plotspeedup{#1_64}{18_64}{tableQuickFallback}
			}
		\end{groupplot}
	\end{tikzpicture}

	\hfil\pgfplotslegendfromname{leg:quick:fallback}\hfil
	\caption{
		Speed-ups of \QS*{} with different thresholds (16--20) for when to fall back to \IS{} over a threshold of 18 elements, on uniformly distributed integers.
		Using \ShS{} is not beneficial because many partitions undercut the thresholds significantly.
	}
	\label{fig:quick:fallback}
\end{figure}


\paragraph{Recursion vs.\ Iteration}
In theory, the question of whether an algorithm should be implemented recursively or iteratively comes down to convenience.
Due to the uniform costs of instructions, jumping to the beginning of a loop or to the beginning of a function essentially costs the same, as does managing arguments automatically through the regular call stack and manually through a simulated one.
Furthermore, in case of \QS{}, the compiler turns tail-recursive calls into jumps back to the beginning of the function, so that one partition is sorted recursively and the other iteratively.
All this would suggest a recursive implementation due to the reduced maintenance.

In practice, it comes down to the compilation.
Even parts of the algorithms which are independent from the choice between recursion and iteration can be compiled differently, such that there are implementations where iteration is faster than recursion and the other way around.
Overall though, iterative implementations tend to be compiled better with superior register usage and less instructions used for the actual \QS{} algorithm.


\paragraph{Partition Prioritisation}
Always sorting the shorter partition first and putting the longer partition on the call stack guarantees that the problem size is at least halved each step, so that the call stack stores \(\bigoh{\log n}\) elements at most.
Unfortunately, this approach is detrimental to the quality of the compilation, which is why it is advisable to always prioritise the same side.
Whether the left-hand or the right-hand partition is sorted first should not make any difference for the runtime but even that changes the quality of the compilation;
in this thesis, the right-hand partitions are prioritised.


\paragraph{Pivot Selection}
Another parameter to tune is the way in which the pivot is selected.
The following were implemented and tested:
\begin{itemize}
	\item
	Using the \emph{last element} is the fastest way to select, requiring zero additional instructions.

	\item
	Taking the \emph{deterministic median} of three elements, namely the first, middle, and last one, is more computationally expensive since the position of the middle element must be calculated, the median be determined, and the pivot be swapped with the last element of the array.

	\item
	A \emph{random element} is most efficiently drawn using an xorshift random number generator and rejection sampling \cite{lukas_geis}.

	\item
	The \emph{random median} is a combination of the previous two methods, where the median of three random elements is taken.
	For simplicity, there is no check on whether an element is drawn twice or thrice.
	Since the partitions are rather long, this should happen seldom, anyhow.
\end{itemize}
A median increases the chances of selecting a pivot that is neither particularly high nor particularly low.
This leads to more balanced partitions such that the call stack is less likely to overflow and the base cases are reached faster.
But as long as one of the deterministic methods is used, it is possible to construct inputs where the runtime climbs up to \(\bigoh*{n^2}\)~\cite{erkiö1984worstcase}, for example when everything is moved to the same partition so that the problem size is reduced by only one element (namely the pivot) after each partitioning step.
This is especially problematic as it easily leads to an overflown call stack.

The random pivots circumvent this issue.
Whilst the pivots could, by ill luck, also lead to the same unbalanced partitions as the deterministic pivots, the worst-case expected runtime is \(\bigoh{n \log n}\) \cite{blum2011probabilistic}.
The median of medians \cite{blum1973median} guarantees a runtime of \(\bigoh{n \log n}\) but was not implemented because a \emph{performant} implementation would probably be quite complex and its benefit minuscule for this thesis.

\input{tasklet_quick_compilation}

\input{tasklet_quick_performance}
