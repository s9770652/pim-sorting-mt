\section{\texorpdfstring{\QS{}}{QuickSort}}
\label{sec:tasklet:quick}

\QS{} \cites{hoare1962quicksort}[88-91]{maurer1974datenstrukturen} uses partitioning to sort in an expected average runtime of \(\bigoh{n \log n}\) and a worst-case runtime of \(\bigoh{n^2}\):
A so-called \emph{pivot} element is chosen from the input array according to some method, then the whole input array gets scanned and elements greater or less than the pivot are moved to the right-hand or left-hand side of the array, that is the \emph{partitions}, respectively;
elements equal to the pivot are allowed to be in either partition.
Finally, \QS{} is used on the left-hand and right-hand partition.

The partitioning is implemented using a variant of \citeauthor{hoare1962quicksort}'s original scheme \cite{hoare1962quicksort}:
After the pivot \lstinline|p| is chosen, two pointers are set to either end of the partition.
The left pointer \lstinline|i| moves rightwards until finding an element at least as great as the pivot (\lstinline|*i >= *p|), while the right pointer \lstinline|j| moves leftwards until finding an element at most as great as the pivot (\lstinline|*j <= *p|).
Are the two elements found unequal, they are in the wrong order, so swapping them puts them in the right partitions.
Are they equal, swapping them anyway does not violate the order.
After swapping the elements, the pointers move onwards as described.
This process of repeated swaps continues until the pointers meet.
The place where they meet marks the end of the left-hand partition and the beginning of the right-hand one.
%Finally, the pivot is swapped with the first element of the right partition.

\QS{} does not sort in-place, as additional space of size \(\bigoh{\log n}\) is needed for a call stack.
Furthermore, \QS{} is not stable.


\paragraph{Sentinel Values}
So as to avoid manifold bounds checks on the pointers, the partitioning presented above does not exactly follow \citeauthor{hoare1962quicksort}'s original scheme, where pointers halted only if \lstinline|*i > *p| and \lstinline|*j < *p|.
Once the pivot \lstinline|p| is chosen, it is swapped with the last element.
This way the left pointer \lstinline|i| halts at the end of the array at the latest, where it definitely holds \lstinline|*i >= *p|.
The left pointer \lstinline|i| also works as sentinel value for the right pointer \lstinline|j|:
Either the left pointer \lstinline|i| halted because of \lstinline|*i == *p|, so the right pointer \lstinline|j| halts at address \lstinline|i| at the latest, or it halted because of \lstinline|*i > *p|.
Then, however, it either passed over the previous address, so it holds \lstinline|*(i - 1) < *p|, or it also halted there.
In the latter case, a swap with an element on which the right pointer \lstinline|j| halted occurred, so it holds \lstinline|*(i - 1) <= *p| afterwards.
In summary, the right pointer \lstinline|j| halts either at address \lstinline|i| or \lstinline|i - 1|.
This means that the only check needed during partitioning is whether it holds \lstinline|j <= i| whenever both pointers halted.
Once the partitioning is over, the pivot is swapped to its final address \lstinline|i|.
The left-hand partition ends at address \lstinline|i - 1|, and the right-hand one begins at address \lstinline|i + 1|.

There is a downside to this modification:
Elements which are equal to the pivot are also swapped during partitioning.
Also, if they happen to neighbour the pivot in its final position, they will be touched again in subsequent partitionings even though they are correctly positioned, too.
Admittedly, to remedy the last point and find tighter partition bounds if many duplicates are known to exists, one could let the pointers \lstinline|i| and \lstinline|j| move onwards after meeting until they find elements unequal to the pivot.

One could omit swapping the pivot to the end of array and instead place an explicit sentinel directly behind the end.
In that case, one would lose the guarantee on where the pivot ends up, and the combined length of the two resulting partitions could not shrink by one element compared to the original array as address \lstinline|i| must still be included.
We did test this version, but the results are unreliable due to reasons shown later in \enquote{\nameref{sec:tasklet:quick:compilation}}.


\paragraph{Base Cases}
When only a few elements remain in a partition, \QS{}'s overhead predominates such that \IS{} lends itself as fallback algorithm.
Up to 40\% of the runtime is saved by falling back.
As seen in \cref{fig:quick:fallback}, the optimal threshold for switching the sorting algorithm is 18 elements for 32-bit integers on uniform inputs and likely similar on inputs following Zipf's or normal distributions.
For 64-bit integers, the optimal threshold is 17 elements.
Notwithstanding, we set 18 elements to be the default threshold for both data types to simplify matters since the impact is minuscule.
For sorted and almost sorted inputs, the threshold is higher since \IS{} performs well on them, so falling back earlier and, thus, ending the sorting process is better.
Because \QS{}'s two pointers invert large swaths of reverse sorted inputs while partitioning, the same is true for that input distribution even though it is \IS{}'s worst-case.
However, such input distributions should be catered for by a pattern-defeating \QS{} as laid out in \cref{sec:tasklet:conclusion}, hence the 18 elements as default threshold altogether.

To avoid unnecessary uses of \IS{}, another base case is imaginable, namely terminating when a partition contains at most one element.
There are tremendous consequences for the runtime depending on the exact implementation of the base cases, as shown later in \enquote{\nameref{sec:tasklet:quick:compilation}}.
\begin{figure}[t]
	\pgfplotstableset{
		create on use/n/.style={create col/copy column from table={data/quick/fallback/uint32/16.txt}{n}},
	}
	\pgfplotsinvokeforeach{14,15,16,17,18,19,20}{
		\pgfplotstableset{create on use/µ_#1_32/.style={create col/copy column from table={data/quick/fallback/uint32/#1.txt}{µ_TrivialBC}}}
		\pgfplotstableset{create on use/µ_#1_64/.style={create col/copy column from table={data/quick/fallback/uint64/#1.txt}{µ_TrivialBC}}}
	}
	\pgfplotstablenew[columns={n,µ_14_32,µ_15_32,µ_16_32,µ_17_32,µ_18_32,µ_19_32,µ_20_32,µ_14_64,µ_15_64,µ_16_64,µ_17_64,µ_18_64,µ_19_64,µ_20_64}]{\pgfplotstablegetrowsof{data/quick/fallback/uint32/16.txt}}{\tableQuickFallback}

	\tikzsetnextfilename{quick_fallback}
	\begin{tikzpicture}[plot]
		\begin{groupplot}[
			horizontal sep for labels,
			adaptive group=1 by 2,
			groupplot ylabel={Speed-up},
			x from 16 to 1024,
			ymin=0.993,
			ymax=1.001,
			extra y ticks={0.993,1.001},
			yticklabel style={/pgf/number format/.cd, precision=3, fixed, zerofill},
		]
			\nextgroupplot[title/.add={}{32-bit}]
			\pgfplotsset{legend to name=leg:quick:fallback, legend entries={15,...,20}}
			\pgfplotsset{update limits=false} \addplot coordinates {(15,0.99)}; \pgfplotsset{update limits=true}
			\pgfplotsinvokeforeach{16,17,18,19,20}{
				\ifnumequal{#1}{18}{
					\addplot coordinates {(18,0.99)};
				}{
					\plotspeedup{#1_32}{18_32}{tableQuickFallback}
				}
			}
			%
			\nextgroupplot[title/.add={}{64-bit}]
			\pgfplotsinvokeforeach{15,16,17,18,19}{
				\ifnumequal{#1}{17}{
					\addplot coordinates {(17,0.99)};
				}{
					\plotspeedup{#1_64}{17_64}{tableQuickFallback}
				}
			}
		\end{groupplot}
	\end{tikzpicture}

	\hfil\pgfplotslegendfromname{leg:quick:fallback}\hfil
	\caption{
		Speed-ups of \QS*{} with different thresholds (15--20) for when to fall back to \IS{} over a threshold of 18 elements (32-bit) and 17 elements (64-bit), conducted on uniform input distributions.
		Using \ShS{} was not beneficial overall, likely because many partitions fall below the thresholds.
	}
	\label{fig:quick:fallback}
\end{figure}


\paragraph{Recursion vs.\ Iteration}
In theory, the question of whether an algorithm should be implemented recursively or iteratively comes down to convenience.
Due to the uniform costs of instructions, jumping to the start of a loop or to the start of a function essentially costs the same, as does managing arguments automatically through the regular call stack and manually through a simulated one.
Furthermore, in case of \QS{}, the compiler turns tail-recursive calls into jumps back to the beginning of the function, so that one partition is sorted recursively and the other iteratively.
All this would suggest a recursive implementation due to the reduced maintenance.

In practice, it comes down to the compilation.
Even parts of the algorithms which are independent from the choice between recursion and iteration can be compiled differently, such that there are implementations where iteration is faster than recursion and the other way around.
Overall though, iterative implementations \emph{tend} to be compiled better with superior register usage and less instructions used for the actual \QS{} algorithm.


\paragraph{Partition Prioritisation}
Whether the left-hand or the right-hand partition is sorted first should not make any difference for the runtime but actually does so because of different compilation, as shown later in \enquote{\nameref{sec:tasklet:quick:compilation}}.
Always sorting the shorter partition first and putting the longer partition on the call stack guarantees that the problem size is at least halved each step, so that the call stack stores \(\bigoh{\log n}\) elements at most.
Unfortunately, this approach is detrimental to the quality of the compilation, which is why it is advisable to always prioritise the same side;
in this thesis, the right-hand partitions are prioritised.
At least, an overflow of the call stack can be made unlikely with the right pivot choice.


\paragraph{Pivot Choice}
Another parameter to tune is the way in which the pivot is chosen.
The following were implemented and tested:
\begin{itemize}
	\item
	Using the \emph{last element} is the fastest way to chose, requiring zero additional instructions.

	\item
	Taking the \emph{deterministic median} of three elements, namely the first, middle, and last one, is more computationally expensive since the position of the middle element must be calculated, the median be determined, and the pivot be swapped with the last element of the array, where it acts as sentinel.

	\item
	A \emph{random element} is most efficiently drawn using an xorshift random number generator and rejection sampling \cite{lukas_geis}.

	\item
	The \emph{random median} is a combination of the previous two methods, where the median of three random elements is taken.
	For simplicity, there is no check on whether an element is drawn twice or thrice.
	Since the partitions are rather long, this should happen seldom, anyhow.
\end{itemize}
A median increases the chances of choosing a pivot that is neither particularly high nor particularly low.
This leads to more balanced partitions such that the call stack is less likely to overflow and the base cases are reached faster.
But as long as one of the deterministic methods is used, it is possible to construct inputs where the runtime climbs up to \(\bigoh*{n^2}\)~\cite{erkiö1984worstcase}, for example when everything is moved to the same partition so that the problem size is reduced by only one element (namely the pivot) after each partitioning step.

The random pivots circumvent this problem.
Whilst the pivots could, by ill luck, also lead to the same unbalanced partitions as the deterministic pivots, the worst-case expected runtime is \(\bigoh{n \log n}\) \cite{blum2011probabilistic}.
The median of medians \cite{blum1973median} guarantees a runtime of \(\bigoh{n \log n}\) but was not implemented because a \emph{performant} implementation would probably be quite complex and its benefit minuscule for this thesis.

\input{tasklet_quick_compilation}

\input{tasklet_quick_performance}
