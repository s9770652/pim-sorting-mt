\subsection{\texorpdfstring{\QS{}}{QuickSort}}
\label{subsec:tasklet:quick}

\pgfplotstablereadnamed{data/quick/fallback/uint32/composite.txt}{tableQuickFallback_32}
\pgfplotstablereadnamed{data/quick/fallback/uint64/composite.txt}{tableQuickFallback_64}

\QS{} uses partitioning to sort in an expected average runtime of \(\bigoh{n \log n}\):
A pivot element is chosen from the input array, then the input array gets scanned and elements greater or lesser than the pivot are moved to the right or left of the pivot, respectively.
Finally, \QS{} is used on the left and right partitions.
The \QS*{} implementations presented here are neither stable nor in-place.


\paragraph{Base Cases}

\begin{figure}
	\begin{tikzpicture}[plot]
		\begin{groupplot}[
			horizontal sep for labels,
			adaptive group=1 by 2,
			groupplot xlabel={Input Length \(n\)},
			groupplot ylabel={Speed-up},
			xmode=log,
			xtick={16, 64, 256, 1024},
			xticklabels={\(16\), \(64\), \(256\), \(1024\)},
			minor xtick={32, 128, 512},
			ymin=0.994,
			ymax=1.001,
			extra y ticks={1.001},
			yticklabel style={/pgf/number format/.cd, precision=3, fixed, zerofill},
			legend columns=-1,
		]
			\nextgroupplot[title=32-bit\strut]
			\pgfplotsset{legend to name=leg:quick:fallback, legend entries={16,17,19,20}}
			\pgfplotsinvokeforeach{16,17,19,20}{
				\plotspeedup{#1}{18}{tableQuickFallback_32}
			}
			%
			\nextgroupplot[title=64-bit\strut]
			\pgfplotsinvokeforeach{16,17,19,20}{
				\plotspeedup{#1}{18}{tableQuickFallback_64}
			}
		\end{groupplot}
	\end{tikzpicture}

	\hfil\pgfplotslegendfromname{leg:quick:fallback}\hfil
	\caption{
		Speed-ups of \QS*{} with different thresholds for when to fall back to \IS{} over a threshold of 18 elements.
		Using \ShS{} was not beneficial overall, likely because many partitions fall below the thresholds.
	}
	\label{fig:quick:fallback}
\end{figure}

When only a few elements remain in a partition, \QS{}'s overhead predominates such that \IS{} lends itself as fallback algorithm.
As seen in \cref{fig:quick:fallback}, the optimal threshold for switching the sorting algorithm is 18 elements for uniform inputs and likely similar for inputs following Zipf's or normal distributions.
For sorted and almost sorted inputs, the threshold is higher since \QS{} does not move elements which are already on the correct side of the pivot so (almost) no changes are made.
Since \IS{} does little more than one scan of these inputs, falling back earlier and, thus, ending the sorting process is better.
For reverse sorted inputs, the threshold is also higher even though these are the worst-case inputs for \IS{}.
The reason lies within the implementation of \QS{} which repeatedly swaps the foremost element greater than the pivot and the hindmost element lesser than the pivot.
This way, reverse sorted inputs are reversed in the very first partition step.
However, these input distributions should be catered for by a pattern-defeating \QS{} as laid out in \todo{Verweis auf sp채ter}, hence the 18 elements as default threshold.

Besides falling back to \IS{}, another base case is imaginable, namely terminating when a partition has a length of at most 1 elements.
Realistically speaking, checking for this should not be necessary, because even though the extra check is doable with just one additional instruction, it occurs rarely, and the \IS{} would terminate after a few instructions anyway.
Yet, there are tremendous consequences for the runtime depending on the exact implementation of the base cases, as seen later in \enquote{\nameref{subsubsec:tasklet:quick:compilation}}.


\paragraph{Recursion vs.\ Iteration}
In theory, the question of whether a DPU algorithm should be implemented recursively or iteratively comes down to convenience.
Due to the uniform costs of instructions, putting arguments automatically on the call stack or manually in an array essentially costs the same, as does jumping to the start of a loop and to the start of a function.
\todo{Kann jump r23 es kaputtmachen?}
Furthermore, in case of \QS{}, the compiler turns tail-recursive calls into jumps back to the function start, so that one partition is sorted recursively and the other iteratively.
All this would suggest a recursive implementation due to less code complexity.

In practice, it comes down to the compilation.
Selcouthly, even parts of the algorithms which are independent from the choice between recursion and iteration can be compiled differently, such that there are implementations where iteration is faster than recursion and the other way around.
Overall though, iterative implementations tend to be compiled better with superior register usage and less instructions used for the actual \QS{} algorithm.
The fastest implementation is indeed an iterative one, even if it beats the fastest recursive implementations \Dash outliers, admittedly \Dash by less than 4\%.
\todo{zur체ckkehren}


\paragraph{Partition Prioritisation}
Whether the left-hand or the right-hand is put on the stack should not make any difference for the runtime.
However, always putting the longer partition on the stack guarantees that the problem size is at least halved each step, meaning the call stack stores \(\bigoh{log n}\) elements at most.
This last approach, as shown later, is linked to huge speed penalties, so always prioritising the same partition is actually used throughout this Thesis in general.
But even then, the choice between the two sides can have tremendous effects.
\todo{zur체ckkehren}


\paragraph{Pivot Choice}
Another parameter to tune is the way in which the pivot is chosen.
The following were implemented and tested:
\begin{itemize}
	\item
	Using the \emph{last element} is the fastest way, requiring zero additional instructions.

	\item
	Taking the \emph{median of three elements}, namely the first, middle, and last one, is far more computationally expensive since the position of the middle element must be calculated, the median be determined, and the pivot be swapped with the last element of the array, where it acts as sentinel.
	The plus side is that this method increases the chances of choosing a pivot that is neither particularly high nor particularly low.
	This leads to more balanced partitions such that the call stack is less likely to overflow and the base cases are reached faster.

	\item
	A \emph{random element} is most efficiently drawn using an xorshift random number generator and rejection sampling \cite{lukas_geis}.
	This takes some instructions but impedes deterministically chosen worst-case inputs.

	\item
	Taking the \emph{median of three random elements} is a combination of the previous two methods.
	For simplicity, there is no check on whether an element is drawn twice or thrice.
	Since the partitions are rather long, this should happen seldom, anyhow.
\end{itemize}



Luckily, the pivot choice seldom has bearing on the overall compilation, making a comparison easier.
\todo{Stimmt nicht!}
The results are shown in \cref{fig:quick:pivot}.
Choosing the middle element is cheap enough for the runtime to be slowed down by a low single-digit percentage, and the increased pivot quality from choosing the median of three elements more than offsets the cost increase, thus making it the best choice.
At 1024 elements, the runtime with a random pivot is 10\% worse than with the median of three elements.
Since drawing the random index is more than thrice as costly as computing the middle index, a median of three random elements would likely yield even worse times, should one need randomisation.
Again, more details are given in \cref{subsubsec:tasklet:quick:compilation}.
\todo{zur체ckkehren}



\input{tasklet_quick_compilation}
