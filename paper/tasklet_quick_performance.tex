\subsection*{Evaluation of the Performance}
\label{sec:tasklet:quick:performance}
\addcontentsline{toc}{subsection}{\nameref{sec:tasklet:quick:performance}}

\pgfplotsinvokeforeach{sorted,reverse,almost,uniform,zipf,normal}{
	\pgfplotstablereadnamed{data/quick/matrix/iterative/Median_of_random/right/uint32/#1.txt}{tableQuickRand_32#1}
	\pgfplotstablereadnamed{data/quick/matrix/recursive/Median_of_random/right/uint64/#1.txt}{tableQuickRand_64#1}
}

Before turning to the performance of \QS{} on specific input distributions, the ratio between costs and benefits of the pivot choices shall be evaluated.
Looking again at \cref{fig:quick:implementations,fig:heap:runtime_uint64} shows that a median gets more beneficial, the longer the input becomes, achieving small pay-offs for the longest ones.
Moreover, the standard deviations of the runtimes, although not shown in the figures for reasons of clarity, are cut roughly in half.
Randomisation slows down noticeably, so random pivots are disadvantageous if the input is known to be fairly random.
However, the decrease remains in the single digits percentage-wise, supporting the findings by \citeauthor{lukas_geis}~\cite{lukas_geis} that drawing random numbers is quite cheap.
For this reason, the random median is used as default method throughout this thesis.

\begin{figure}
	\tikzsetnextfilename{quick_runtime}
	\begin{tikzpicture}[plot]
		\begin{groupplot}[
			adaptive group=1 by 2,
			groupplot ylabel={Cycles / \((n \lb n)\)},
			x from 16 to 1024,
			ytick distance=10,
		]
			\nextgroupplot[title/.add={}{32-bit}, ymin=30, ymax=80]
			\pgfplotsset{legend to name=leg:quick:runtime, legend entries={Sorted, Reverse S., Almost S., Uniform, Zipf's, Normal}}
			\pgfplotsinvokeforeach{sorted,reverse,almost,uniform,zipf,normal}{
				\plotpernlogn{TrivialBC}{tableQuickRand_32#1}
			}
			%
			\nextgroupplot[title/.add={}{64-bit}, ymin=40,ymax=90]
			\pgfplotsinvokeforeach{sorted,reverse,almost,uniform,zipf,normal}{
				\plotpernlogn{TrivialBC}{tableQuickRand_64#1}
			}
		\end{groupplot}
	\end{tikzpicture}

	\hfil\pgfplotslegendfromname{leg:quick:runtime}\hfil
	\caption{
		Mean runtime of \QS{} on all tested input distributions and data types.
	}
	\label{fig:quick:runtime}
\end{figure}

\Cref{fig:quick:runtime} shows the runtime of \QS{} in it default configuration, that is, with random medians.
\Cref{fig:quick:runtime_uint32,fig:quick:runtime_uint64} additionally contain the runtimes with deterministic medians as well as the standard deviations of the measurements.
The mean runtimes are rather close across all input distributions, a consequence of using random medians and of considering elements equal to the pivot as different.
In fact, it is \IS{} that primarily causes the discrepancies, as setting the threshold to one element proves.
This also explains why \QS{} performs so well on large inputs with Zipf's distribution:
This distributions generates many duplicates, which are put into the same partitions, so \IS{} performs many simple scans.

One might expect \QS{} to perform even better on sorted and reverse sorted input, since everything is either already in the correct position or because the two pointers quickly invert large swaths of the inputs.
However, a side effect of swapping the pivot twice can be that many elements are displaced by one position from where they should be in the sorted input.
Take reverse sorted inputs and the deterministic median as an example:
The element \(n/2\) is chosen as pivot out of the elements \(n\), \(n/2\), and \(0\) and then gets swapped with the last element, that is, with \(0\).
Thereupon, the pointers invert the rest of the input such that the start of the input looks something like \(1, 2, \dots, n/2-1, 0, n/2, \dots\) after the first partitioning step.
Indeed, this pattern makes \QS{} with deterministic medians degrade and eventually overflow the call stack, which is why the respective plots in \cref{fig:quick:runtime_uint32,fig:quick:runtime_uint64} leave the charts.
An implementation without swapping the pivot promises better performance for such cases, but in exploratory ones, the performance on more random input distributions suffered drastically.
