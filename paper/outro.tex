\chapter{Conclusion}
\label{sec:conclusion}

In this thesis, we engineered several sequential and parallel integer sorting algorithms utilising \acf{PIM} on \upmem{}-based \acfp{DPU}.
\Acp{DPU} are threaded general-purpose processors which reside on the memory banks of \acs{DRAM} modules.
Due to this spatial proximity, the latency of memory accesses is greatly reduced.
However, the computational capabilities of a \ac{DPU} are low.
The memory hierarchy of a \ac{DPU} is two-tiered, entailing a small but fast scratch pad memory called \acf{WRAM} and a larger but slower \acf{MRAM}.

In \cref{sec:tasklet}, we focused on sequentially sorting data which fits entirely into the \ac{WRAM}.
A crucial means to optimise the performance of a \ac{DPU} algorithm proved to be the reduction of the instruction count, as all instructions are executed in the same amount of time and \ac{WRAM} accesses are uniform.
Often, this included the exploitation of sentinel values to reduce bounds checks and loop unrolling to lessen loop overheads.
We found \QS{} to deliver a well-rounded performance across all benchmarked input distributions.
The performance by \MS{} is strong as well, mostly following suit and in some cases even surpassing \QS{}.
\HS{} proved to be uncompetitive.
Both \QS{} and \MS{} make use of \IS{}, which we found to perform well on short inputs with about 16 elements or fewer.

In \cref{sec:mram}, we focused on sorting data which have to be stored in the \ac{MRAM} for size reasons.
We proposed sorting algorithms both for sequential execution by a single tasklet and parallel execution by an entire \ac{DPU}.
These algorithms are based on \MS{} to allow for stable sorting, but there are also unstable variants which are faster than their stable counterparts.
We designed an elaborate two-tier merge procedure to lower the instruction count.
An additional challenge was the reuse of allocated memory and the management of data transfers between the \ac{MRAM} and the \ac{WRAM}.
For the latter, \upmem{} provides a software utility called sequential reader, which we optimised to gain a further speedup of \num{1.6}.
In case of the parallel sorting algorithm, communication and synchronisation were implemented using shared memory.
The parallel speedup achieved ranges from nearly hitting the optimum to reaching just half of it, depending on the input distribution.

There is still room for improvement of the proposed sorting algorithms.
The compiler produces suboptimal code in quite a few instances of which we have noted several throughout this thesis.
\QS{} suffers the most from this and its current implementation has shortcomings whose removal may culminate in the use of inline assembler.
A more optimised \QS{} might manage to beat \MS{} more clearly.
The compiler issues were an additional reason not to design a \QS{} which works on \ac{MRAM} data.
We conjecture that such an \ac{MRAM} \QS{} would be very performant due to the uniform cost of instructions and memory accesses.
Our optimised sequential reader could aid with the latter, as it is trivially expanded to support the two different directions in which \QS{} reads data.
Furthermore, own cursory experiments have shown that sorting networks are a strong contender to \IS{}.
More suggestions concerning the sequential sorting algorithms can be found in \cref{sec:tasklet:conclusion}.
Last but not least, we believe that the suboptimal parallel speedup in some cases is remediable by a technique for more balanced workloads which we sketched in \cref{sec:mram:par:aspects}.

Currently, the implementations of the sorting algorithms support only 32-bit and 64-bit integers.
Supporting shorter integers or compound data types would probably complicate memory alignment, but ultimately, the performance should remain largely the same.
Supporting floating point numbers, however, would likely hurt the performance irremediably.
The reason is that \acp{DPU} have no hardware support for floating point arithmetic, so comparisons between floating point numbers require emulation in software.

Then next greater step in sorting integers would be the employment of multiple \acp{DPU}.
This poses new challenges for communication and synchronisation as there is no direct communication channel between \acp{DPU}, much less shared memory.
Instead, any inter-\acs{DPU} communication must be overseen by a host \ac{CPU}.
Moreover, \acp{DPU} are grouped together, and the host cannot access a \ac{DPU} until all \acp{DPU} of its group have finished executing.
This introduces an inherent lockstep to the execution, thereby hurting performance.
