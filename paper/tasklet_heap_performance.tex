\subsection{Evaluation of the Performance}
\label{sec:tasklet:heap:performance}

\expandafter\pgfplotsinvokeforeach\expandafter{\alldists}{
	\pgfplotstablereadnamed{data/heap/uint32/#1.txt}{tableHeap_32#1}
	\pgfplotstablereadnamed{data/heap/uint64/#1.txt}{tableHeap_64#1}
}

\pgfplotsset{
	heap/.style={
		adaptive group=1 by 2,
		groupplot ylabel={Cycles / \((n \lb n)\)},
		x from 16 to 1024,
	},
}

\def\heapalgos{HeapOnlyDown,HeapUpDown,HeapSwapParity}

\begin{figure}
	\tikzsetnextfilename{heap_runtime}
	\begin{tikzpicture}[plot]
		\begin{groupplot}[heap, ytick distance=5]
			\nextgroupplot[title/.add={}{32-bit}, ymin=130, ymax=155, legend to name=leg:heap:runtime]
			\legend{\HS{} (top-down), \HS{} (bottom-up), \HS{} (swap parity)}
			\expandafter\pgfplotsinvokeforeach\expandafter{\heapalgos}{
				\plotpernlogn{#1}{tableHeap_32uniform}
			}
			%
			\nextgroupplot[title/.add={}{64-bit}, ymin=155, ymax=180]
			\expandafter\pgfplotsinvokeforeach\expandafter{\heapalgos}{
				\plotpernlogn{#1}{tableHeap_64uniform}
			}
		\end{groupplot}
	\end{tikzpicture}

	\tikzexternaldisable\hfil\pgfplotslegendfromname{leg:heap:runtime}\hfil\tikzexternalenable
	\caption{
		Mean runtimes of all \HS{} implementations on uniformly distributed integers.
	}
	\label{fig:heap:runtime}
\end{figure}

The measurements are visualised in \cref{fig:heap:runtime,fig:heap:runtime_uint32,fig:heap:runtime_uint64}.
In general, the performance of \HS{} is hardly volatile and mostly independent from the input distribution.
Reverse sorted inputs are sorted faster than most since they are already max-heaps so the heap-building phase is brief, whereas sorted inputs are sorted the slowest since they are min-heaps essentially needing inversion.
Nonetheless, the reverse sorted inputs get sorted at most \qty{10}{\percent} faster than the sorted ones.
An extra√∂rdinary outlier are zero-one inputs on which the top-down \HS{} achieves a speedup of \num{1.8} over the bottom-up \HS{}.
With zero-one inputs, roughly half of the elements are zeroes and roughly half are ones.
After building the min-heap, the first half of the array consists of ones and the second half of zeroes.
About half of the ones and about half of the zeroes are already in these respective halves of the input, so the heap-building phase is somewhat brief, too.
More importantly, however, is the foreshortening of the downwards-sifting.
After about \(n/2\) many rounds, only zeroes remain in the heap.
Therefore, no last leaf violates the heap order when moved to the top, so the downwards-sifting terminates immediately.
But the downwards-sifting becomes briefer even earlier as many ones turn into leaves once the zeroes which are their children have been moved to the top and were sifted down to somewhere else.
Such ones do not violate the heap order when moved to the top at a later point, too.


Ignoring this outlier, the normalised runtimes of the top-down \HS{} and the bottom-up \HS{} with swap parity show a slight upwards trends, whereas that of the bottom-up \HS{} with swap disparity mostly shows a slight downwards trends.
The exception are reverse sorted inputs, where the latter also shows a slight upwards trend.
Of interest is their ranking:
Value checks on 64-bit integers take two instructions, so that the savings of the bottom-up \HS{} with swap disparity allow it to outperform the top-down \HS{} even for short inputs.
Its advantage grows with the input length.
This makes sense as roughly \qty{50}{\percent} of the vertices are leaves and \qty{25}{\percent} are parents of leaves, no matter the total heap size.
Therefore, the percentage of former last leaves being sifted down from the top to the bottom remains steady but the travelled distance increases.
Value checks on 32-bit integers, on the other hand, take only one instruction, so that the reduction of these is overshadowed by the increased overhead from the longer downwards-sifting and the added upwards-sifting.
Indeed, at around 2000 elements, the bottom-up \HS{} with swap disparity overtakes the top-down \HS{} because of their inverse trends, but the lead stays meagre even at \num{6000} elements.
%However, these long inputs are uninteresting due to the limited \ac{WRAM} and the multiple tasklets used in most applications.

The bottom-up \HS{} with swap parity consistently trails behind.
This comes as no surprise since the overhead of its considerably prolonged upwards-sifting bears no proportion to the few swaps saved.
This holds true even for 64-bit integers as moves still cost only one instruction so the savings do not increase.
Unrolling the upwards-sifting (cf.\ \cref{sec:tasklet:merge}) proved to be unhelpful.
