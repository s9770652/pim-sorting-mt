\section{\texorpdfstring{\MS{}}{MergeSort}}
\label{sec:tasklet:merge}

\MS{} repeatedly compares two sorted subarrays and merges them into a longer sorted array in time \(\bigtheta{n \log n}\).
Unlike \QS{}, this runtime is guaranteed at the cost of even more needed additional space of size \(\bigtheta{n}\).
Besides \IS{}, \MS{} is the only stable sorting algorithm presented in this section.

\paragraph{Starting Runs}
Instead of starting by merging runs of length 1, it is beneficial to first create longer starting runs using either \IS{} or \ShS{}.
An important downside to \ShS{} is that while it does allow to sort bigger starting runs quicker, it is not stable unlike \IS{}.
If \MS{} is supposed to sort stably, then \IS{} has to be used.

Unlike \QS{}, where each partition naturally acted as sentinel for the subsequent one, it is necessary to temporarily place sentinels values in front of each starting run and later restore the original values of the preceding run.
The step sizes used for \ShS{} \Dash namely \(\stepsizes = (6, 1)\) for lengths up to 48 elements, and \(\stepsizes = (12, 5, 1)\) for everything above \Dash have been chosen based on the results in \cref{sec:tasklet:shell}, according to which these step sizes offer top performance for uniformly distributed inputs and medial performance for the reverse sorted inputs.
Spot-check inspection suggest no deterioration of \IS{}'s and \ShS{}'s compilation due to inlining.

\paragraph{Memory Footprint}
A simple but fast implementation of \MS{} writes all merged runs to an auxiliary array, raising the need for space for \(n\) additional elements (\enquote{full-space}).
After a round is finished and all pairs of runs have been merged, the input array and the auxiliary array switch roles, and the merging starts anew.
Are the final sorted elements supposed to be saved in the original input array, a final round with a write-back from the auxiliary array to the input array is needed for some input lengths.

A slightly more sophisticated implementation needs space for only \(n/2\) additional elements (\enquote{half-space}):
When two adjacent runs are to be merged, the first one can be copied to an auxiliary array.
Then, the copy and the second run are merged to the start of the first run.
As a side effect, no write-back is ever needed and, additionally, the merging of two runs can be terminated prematurely once the last element of the copied run is merged, since the last elements of the other run are already in place.
%As a consequence, flushes will only be performed on at most half of the runs.
Further optimised, \MS{} would not need to copy the first runs immediately.
It suffices to search for the foremost element of the first run which is greater than the first element of the second element.
All previous elements are already in the correct position so only the following elements need to be copied to the auxiliary array.
This optimisation, although examined during development, was not in use when measuring runtimes since it unfortunately complicates another optimisation, namely unrolling.

\paragraph{Unrolling}
There are four common reasons for \emph{flushing}, that is, writing \Dash many oftwhiles \Dash consecutive elements:
\begin{enumerate}
	\item
	When two runs are merged and the end of one of them is reached, the remaining elements of the other one can be moved safely to the output location.
	Especially with the sorted, reverse sorted, and almost sorted input distributions, the number of remaining elements will be high.

	\item
	The number of runs is odd, so the full-space \MS{} moves the last run to the output location immediately.

	\item
	The full-space \MS{} may write all elements from the auxiliary array back to the input array if the former contains the final sorted sequence.

	\item
	The half-space \MS{} copies runs, whose length are always a multiple of the the starting run length, before each merger of pairs.
\end{enumerate}
Therefore, flushing account for a considerable part of the runtime, and reducing the loop overhead (variable incrementation and bounds checking) is helpful.
This can be done via \emph{unrolling}:
As long as at least, let us say, \(x\) elements still need to be flushed, the \(x\) foremost elements are moved first and then all necessary variables are incremented by \(x\).
Is \(x\) a compile-time constant, the compiler implements the moving of the elements through \(x\) instruction which use constant, pre-calculated offsets.
Once less than \(x\) elements remain, an ordinary loop which moves elements individually is used.
In good cases, this approach reduces the loop overhead to an \(x\)th, whilst in bad cases, where less than \(x\) are to be flushed, the overhead is increased by one additional check.

Due to time reasons, we refrained from doing automatic and extensive tests and relied on manual and exploratory tests to come up with the following strategy:
When the full-space \MS{} performs a write-back or when the half-space \MS{} copies the first run, \(x\) is set to the starting run length.
In all other cases, \(x\) is set to 24.
This strategy, albeit not optimal, makes the \MS*{} significantly faster:
Sorting sorted, reverse sorted, and almost sorted inputs sees speed-ups up to 30\%, whereas sorting more random inputs still sees speed-ups for the most part and slow-downs into low single-digits at worst, depending on the starting run length.

\input{tasklet_merge_compilation}

\input{tasklet_merge_performance}
