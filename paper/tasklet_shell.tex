\section{\texorpdfstring{\ShS{}}{ShellSort}}
\label{sec:tasklet:shell}

\IS{} suffers from little elements in the back of the input, since those have to be brought to the front through \(\bigtheta{n}\) comparisons and swaps.
\ShS{}~\cites{Shell1959AHS}[Chapter~2.2.4]{wirth1975algorithmen} circumvents this by doing \(k\) passes of \IS{} with decreasing step sizes:
In pass~\(p = 1, \dots, k\) with step size \(\stepsizes_{k - p}\), the input array is divided into \(\stepsizes_{k - p}\) subarrays so that the \(i\)th subarray contains the elements with indices \(\paren{i, \: i + \stepsizes_{k - p}, \: i + 2 \stepsizes_{k - p}, \: \dots}\), for \(0 \le i < \stepsizes_{k - p}\).
These subarrays then get sorted individually through \IS{}.
The final step size is \(\stepsizes_0 = 1\) such that a regular \IS{} is performed.
Intuitively, early \IS*{} are fast as they touch only few elements and little elements in the back are brought forward in large strides.
Later \IS*{} are also fast as elements are close to being sorted.
Like regular \IS{}, \ShS{} also works in place but loses the stability property.

Finding the right balance between the heightened overhead through multiple \IS{} passes and the shortened runtime of each \IS{} pass is subject to research to this day \cite{skean2023optimization,lee2021empirically} and depends on the cost of the operation types (comparing, swapping, looping).
Traditionally, step sizes were constructed mathematically, allowing to determine \ShS{}'s runtime to be, for example, \(\bigoh[\big]{n^{1.2}}\)~\cite[106]{wirth1975algorithmen} or \(\bigoh[\big]{n \log^2 n}\)~\cite[Section 2]{skean2023optimization}, that is better than \IS{}.
Nowadays, well-performing step sizes are identified empirically~\cite{10.1007/3-540-44669-9_12,skean2023optimization,lee2021empirically}, making a generalisation and, thus, asymptotic analysis more difficult.

\input{tasklet_shell_performance}
