\subsection{Structure of an \texorpdfstring{\upmem{}}{UPMEM} Chip}
\label{sec:prereq:arch:structure}

\begin{figure}
	\centering
	\tikzsetnextfilename{arch_chip}
	\begin{tikzpicture}[
		sketch,
		dpu/.style={ fill=black!5 },
		mem d/.style={ fill=accentcolor!10!white },
		mem u/.style={ fill=accentcolor!25!white },
		flow/.style={ {Straight Barb[width=1.5mm]}-{Straight Barb[width=1.5mm]} },
		flow left/.style={ {Straight Barb[width=1.5mm]}- },
		flow right/.style={ -{Straight Barb[width=1.5mm]} },
	]
		\def\lenx{24}
		\def\pad{0.5}
		\def\numdpu{4}
		\def\paddpu{0.35}
		\def\paddpuflow{(0.75)}
		\def\intermem{1}
		\def\lendpux{(\lenx-2*\pad-\numdpu*\paddpu)}
		\def\lendpuy{(5)}
		\def\lencontrolx{((\lenx-2*\pad-\intermem)/2)}
		\def\lencontroly{1}
		\def\leny{(\lendpuy+\lencontroly+2*\pad+\numdpu*\paddpu+1)}

		% DPUs.
		%			\draw (0, 0) rectangle +(\lenx, \leny);

		\coordinate (dpu) at (\pad, \pad);
		\draw[dpu] (dpu) rectangle +({\lendpux}, {\lendpuy});
		\foreach \i in {\numdpu,...,0}{
			\draw[dpu, opacity=(\numdpu+1-\i)/(\numdpu+1)] ($(dpu) + (\i*\paddpu, \i*\paddpu)$) coordinate (dpu\i) rectangle +({\lendpux}, {\lendpuy});
		}
		\draw[latex-latex] ($(dpu) +  ({\lendpux+\pad/2}, 0)$) -- +(\numdpu*\paddpu, \numdpu*\paddpu) node[midway, below right=-1mm] {×8};
		\node[above, opacity=0] at (dpu) {×8};

		% Memories.
		\def\padmem{\paddpu}
		\def\lenmemtotalx{(\lendpux-2*\padmem-3*\intermem)}
		\def\lenmramx{(\lenmemtotalx/2)}
		\def\lenmemx{((\lenmemtotalx-\lenmramx)/3)}
		\def\lenmemy{(\lendpuy-2*\padmem)}
		\def\lenmemhalfy{((\lendpuy-2*\padmem-\intermem)/2)}

		\coordinate (mem) at ($(dpu) + (\padmem, \padmem)$);
		\draw[mem u] (mem)                                                        coordinate (pipe) rectangle +( {\lenmemx},     {\lenmemy}) node[midway] {Pipeline};
		\draw[mem u] ($(mem) + ({\lenmemx+\intermem}, 0)$)                        coordinate (wram) rectangle +( {\lenmemx}, {\lenmemhalfy}) node[midway] {WRAM};
		\draw[mem u] ($(mem) + ({\lenmemx+\intermem}, {\lenmemhalfy+\intermem})$) coordinate (iram) rectangle +( {\lenmemx}, {\lenmemhalfy}) node[midway] {IRAM};
		\draw[mem u] ($(mem) + ({2*(\lenmemx+\intermem)}, 0)$)                    coordinate (dma)  rectangle +( {\lenmemx},     {\lenmemy}) node[midway, align=center] {DMA\\Engine};
		\draw[mem d] ($(mem) + ({3*(\lenmemx+\intermem)}, 0)$)                    coordinate (mram) rectangle +({\lenmramx},     {\lenmemy}) node[midway] {MRAM};

		% Data flows.
		\draw[flow]       ($(iram) + (0, {\lenmemhalfy/2})$)          -- +(-\intermem, 0);  % Pipeline ↔ IRAM
		\draw[flow]       ($(wram) + (0, {\lenmemhalfy/2})$)          -- +(-\intermem, 0);  % Pipeline ↔ WRAM
		\draw[flow right] ($(pipe) + ({\lenmemx}, {\lenmemy/2})$)     -- +({2*\intermem+\lenmemx}, 0);  % Pipeline ↔ WRAM

		\draw[flow]       ($(wram) + ({\lenmemx}, {\lenmemhalfy/2})$) -- +(\intermem, 0);  % WRAM ↔ DMA
		\draw[flow left]  ($(iram) + ({\lenmemx}, {\lenmemhalfy/2})$) -- +(\intermem, 0);  % IRAM → DMA

		\draw[flow]       ($(dma)  + ({\lenmemx}, {\lenmemy/2})$)     -- +(\intermem, 0);  % DMA ↔ MRAM

		\draw[flow]       ($({\pad+\lencontrolx}, {\leny-\pad-\lencontroly/2})$)     -- +(\intermem, 0);  % CSI ↔ DDR
		\foreach \i in {\numdpu,...,0}{
			\pgfmathsetmacro{\fade}{int( (\numdpu+1-\i)/(\numdpu+1) * 100 )}
			\def\flowlen{\leny-2*\pad-\lencontroly-\i*\paddpu-\lendpuy+\padmem}
			\draw[flow, draw=black!\fade] ($(dpu\i) + ({\paddpu+\lenmemx/2+\i*\paddpuflow}, {\lendpuy-\padmem})$) -- +($(0, {\flowlen})$);
			\draw[flow, draw=black!\fade] ($(dpu\i) + ({\lendpux-\padmem-\lenmramx+\lenmemx*1.5+\i*\paddpuflow}, {\lendpuy-\padmem})$) -- +($(0, {\flowlen})$);
		}

		% Controllers.
		\draw[mem u] (\pad, {\leny-\pad})                      coordinate (csi) rectangle +({\lencontrolx}, -\lencontroly) node[midway] {Control/Status Interface};
		\draw[mem d] ($(csi) + ({\intermem+\lencontrolx}, 0)$) coordinate (ddr) rectangle +({\lencontrolx}, -\lencontroly) node[midway] {DDR\liningnums{4} Interface};
	\end{tikzpicture}

	\caption{
		The structure of a \ac{PIM} chip.
		The bright components are part of a standard \ac{DDR} package, the dark components are exclusive to \ac{PIM} chips.
	}
	\label{fig:arch:chip}
\end{figure}

A \ac{PIM} chip (\cref{fig:arch:chip}) contains eight \ac{DRAM} banks of \qty{64}{\mebi\byte} each.
These are connected with a regular memory controller through which a host \ac{CPU} can access the memory.
Next to each \ac{DRAM} bank is a \ac{DPU}.
The eight \acp{DPU} are connected with a special control interface which, in turn, is connected with the memory controller.
This allows the host to communicate with the \acp{DPU} but it does not allow \acp{DPU} to access \ac{DRAM} banks other than their own.
Instead, a \ac{DPU} possesses a direct connection to its \ac{DRAM} bank, thus bypassing the memory controller.
Such an access is also called \acfi{DMA} and is handled by the so-called \emph{\ac{DMA} engine}.
It is not possible for a \ac{DPU} and the host to access a \ac{DRAM} bank concurrently.

\Acp{DPU} contain several major and minor memories.
The memory of the \ac{DRAM} bank is also referred to as \acfi{MRAM}.
It is by far the largest memory of a \ac{DPU} and typically holds both the input provided by the host and the output calculated by the \ac{DPU}.
However, the \ac{MRAM} is also the slowest memory, for each access comes with non-negligible latency.

The \acfi{WRAM} is far smaller, comprising only \qty{64}{\kibi\byte}, yet the latency is practically zero.
A typical workflow is, hence, to load input data from the \ac{MRAM} into the \ac{WRAM}, process it, and write the output data back into the \ac{MRAM}.
The \ac{WRAM} also contains the stacks of the tasklets, where their local variables are stored, and global variables which are visible to all tasklets.
Moreover, tasklets can dynamically allocate further memory, which is also located in the \ac{WRAM}.
A \ac{DPU} does not dispose of a multilevel cache hierarchy moving data automatically like a \ac{CPU} does, and it is in the responsibility of the programmer to ensure that critical data are stored in the \ac{WRAM}.
Anyway, there is still a small number of automatically managed registers (see also \cref{sec:prereq:arch:isa}).
The host may access a specific section of the \ac{WRAM} only if the data has been specifically designated for this purpose, and such transfers are slower than transfers with the \ac{MRAM}.

Whilst the \ac{WRAM} holds the data which is processed, the \acfi{IRAM} contains the program (also called \emph{kernel}) which a \ac{DPU} executes.
The \ac{IRAM} has a size of \qty{24}{\kibi\byte} which translates to a maximum of \num{4096} instructions out of which a kernel has to be built.
This memory can be modified only by the host, as the \ac{DPU} can only read it, which is an automated process usually.

Next to these major memories, there is also a \qty{256}{\bit} large \emph{atomic memory} whose bits are accessible in a thread-safe way, allowing for mutual exclusion, barriers, and the like.
Furthermore, there is a \emph{run memory} through which threads can be booted, suspended, and resumed.

\begin{note}
	There are two \ac{DPU} models, v1A and v1B.
	The former runs at \qty{350}{\mega\hertz} and is equipped with 24 threads, whereas the latter runs at \qty{400}{\mega\hertz} and is equipped with 16 threads.
	Additionally, the model v1B can hold \qty{2}{\kibi\byte} of data less in its \ac{MRAM} and 128 instructions less in its \ac{IRAM}, since parts of those are \textquote{reserved for production and quality control purposes.}~\cite[Introduction~-- DPU chip characteristics]{upmemSDK}
	The model used for the measurements of this thesis is v1A.
\end{note}
