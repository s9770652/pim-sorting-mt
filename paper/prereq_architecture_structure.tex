\subsection{Structure of a \texorpdfstring{\abb{PIM}}{PIM} Chip}
\label{sec:prereq:arch:structure}

\begin{figure}
	\centering
	\tikzsetnextfilename{arch_chip}
	\begin{tikzpicture}[
		sketch,
		dpu/.style={ },
		mem d/.style={ fill=accentcolor!10!white },
		mem u/.style={ fill=accentcolor!25!white },
		flow/.style={ {Straight Barb[width=1.5mm]}-{Straight Barb[width=1.5mm]} },
		flow left/.style={ {Straight Barb[width=1.5mm]}- },
		flow right/.style={ -{Straight Barb[width=1.5mm]} },
	]
		\def\lenx{25}
		\def\pad{0.5}
		\def\numdpu{4}
		\def\paddpu{0.45}
		\def\paddpuflow{\paddpu}
		\def\intermem{1}
		\def\lendpux{(\lenx-2*\pad-\numdpu*\paddpu)}
		\def\lendpuy{(5)}
		\def\lencontrolx{((\lenx-2*\pad-\intermem)/2)}
		\def\lencontroly{1}
		\def\leny{(\lendpuy+\lencontroly+2*\pad+\numdpu*\paddpu+1)}

		% DPUs.
%		\draw (0, 0) rectangle +(\lenx, \leny);

		\coordinate (dpu) at (\pad, \pad);
		\draw[dpu] (dpu) rectangle +({\lendpux}, {\lendpuy});
		\foreach \i in {\numdpu,...,0}{
			\pgfmathsetmacro{\fade}{int( (\numdpu+1-\i)/(\numdpu+1) * 100 )}
			\pgfmathsetmacro{\fadefill}{int( (\numdpu+1-\i)/(\numdpu+1) * 0.05 * 100 )}
			\draw[dpu, draw=black!\fade, fill=black!\fadefill] ($(dpu) + (\i*\paddpu, \i*\paddpu)$) coordinate (dpu\i) rectangle +({\lendpux}, {\lendpuy});
		}
		\draw[latex-latex] ($(dpu) +  ({\lendpux+\pad/2}, 0)$) -- +(\numdpu*\paddpu, \numdpu*\paddpu) node[midway, below right=-1mm] {×8};
		\node[above, opacity=0] at (dpu) {×8};

		% Memories.
		\def\padmem{\paddpu}
		\def\lenmemtotalx{(\lendpux-2*\padmem-3*\intermem)}
		\def\lenmramx{(\lenmemtotalx/2)}
		\def\lenmemx{((\lenmemtotalx-\lenmramx)/3)}
		\def\lenmemy{(\lendpuy-2*\padmem)}
		\def\lenmemhalfx{((\lenmemx-\intermem)/2)}
		\def\lenmemhalfy{((\lendpuy-2*\padmem-\intermem)/2)}

		\coordinate (mem) at ($(dpu) + (\padmem, \padmem)$);
		\draw[mem u] ($(mem)  + (                       0,                        0)$) coordinate (regf) rectangle +({\lenmemhalfx},     {\lenmemy}) node[midway, rotate=90] {Re\smash{g}ister File};
		\draw[mem u] ($(regf) + ({\intermem+\lenmemhalfx},                        0)$) coordinate (pipe) rectangle +({\lenmemx},         {\lenmemy}) node[midway]            {Pipeline};
		\draw[mem u] ($(pipe) + ({\lenmemx+\intermem},                            0)$) coordinate (wram) rectangle +({\lenmemx},     {\lenmemhalfy}) node[midway]            {WRAM};
		\draw[mem u] ($(pipe) + ({\lenmemx+\intermem},     {\lenmemhalfy+\intermem})$) coordinate (iram) rectangle +({\lenmemx},     {\lenmemhalfy}) node[midway]            {IRAM};
		\draw[mem u] ($(wram) + ({\lenmemx+\intermem},                            0)$) coordinate (dma)  rectangle +({\lenmemhalfx},     {\lenmemy}) node[midway, rotate=90] {DMA En\smash{g}ine};
		\draw[mem d] ($(dma)  + ({\lenmemhalfx+\intermem},                        0)$) coordinate (mram) rectangle +({\lenmramx},        {\lenmemy}) node[midway]            {MRAM};

		% Data flows.
		\draw[flow]       ($(pipe) + (0,                  {\lenmemy/2})$) -- +(-\intermem,             0);  % Pipeline ↔ Register File
		\draw[flow]       ($(iram) + (0,              {\lenmemhalfy/2})$) -- +(-\intermem,             0);  % Pipeline ↔ IRAM
		\draw[flow]       ($(wram) + (0,              {\lenmemhalfy/2})$) -- +(-\intermem,             0);  % Pipeline ↔ WRAM
		\draw[flow right] ($(pipe) + ({\lenmemx},         {\lenmemy/2})$) -- +({2*\intermem+\lenmemx}, 0);  % Pipeline ↔ DMA

		\draw[flow]       ($(wram) + ({\lenmemx}, {\lenmemhalfy/2})$) -- +(\intermem, 0);  % WRAM ↔ DMA
		\draw[flow left]  ($(iram) + ({\lenmemx}, {\lenmemhalfy/2})$) -- +(\intermem, 0);  % IRAM → DMA

		\draw[flow]       ($(dma)  + ({\lenmemhalfx}, {\lenmemy/2})$) -- +(\intermem, 0);  % DMA ↔ MRAM

		\draw[flow]       ($({\pad+\lencontrolx}, {\leny-\pad-\lencontroly/2})$) -- +(\intermem, 0);  % CSI ↔ DDR
		\foreach \i in {\numdpu,...,0}{
			\pgfmathsetmacro{\fade}{int( (\numdpu+1-\i)/(\numdpu+1) * 100 )}
			\def\flowlen{\leny-2*\pad-\lencontroly-\i*\paddpu-\lendpuy+\padmem}
			\draw[flow, draw=black!\fade] ($(pipe) + ({\lenmemx/2},  {\lenmemy}) + ({\i*\paddpuflow}, {\i*\paddpu})$) -- +($(0, {\flowlen})$);  % Pipeline ↔ Control/Status Interface
			\draw[flow, draw=black!\fade] ($(mram) + ({\lenmramx/2}, {\lenmemy}) + ({\i*\paddpuflow}, {\i*\paddpu})$) -- +($(0, {\flowlen})$);  % MRAM ↔ DDR4 Interface
		}

		% Controllers.
		\draw[mem u] (\pad, {\leny-\pad})                      coordinate (csi) rectangle +({\lencontrolx}, -\lencontroly) node[midway] {Control/Status Interface};
		\draw[mem d] ($(csi) + ({\intermem+\lencontrolx}, 0)$) coordinate (ddr) rectangle +({\lencontrolx}, -\lencontroly) node[midway] {DDR\liningnums{4} Interface};
	\end{tikzpicture}

	\caption{
		The structure of a \ac{PIM} chip.
		The bright components are part of a standard \ac{DDR} package, the dark components are exclusive to \ac{PIM} chips.~\cite{upmem2021WhitePaper}
	}
	\label{fig:arch:chip}
\end{figure}

A \ac{PIM} chip (\cref{fig:arch:chip}) contains eight \ac{DRAM} banks of \qty{64}{\mebi\byte} each.
These are connected with a regular \ac{DDR} interface through which a host \ac{CPU} can access the memory.
Next to each \ac{DRAM} bank, there is a \ac{DPU} which possesses a direct connection to it, thus bypassing the \ac{DDR} interface.
Such an access is also called a \acfi{DMA} and is handled by the \emph{\ac{DMA} engine}.
Furthermore, the eight \acp{DPU} are connected with a special control interface which, in turn, is connected with the memory controller.
This allows the host to communicate with the \acp{DPU} but it does not allow \acp{DPU} to access \ac{DRAM} banks other than their own.
It is not possible for a \ac{DPU} and the host to access a \ac{DRAM} bank concurrently.

\Acp{DPU} contain several major and minor memories.
The memory of the \ac{DRAM} bank is also referred to as \acfi{MRAM}.
It is by far the largest memory of a \ac{DPU} and typically holds both the input provided by the host and the output calculated by the \ac{DPU}.
However, the \ac{MRAM} is also the slowest memory, for each \ac{DMA} comes with a latency of dozens of cycles.
Also, despite the theoretical maximum bandwidth of \qty{17.9}{\gibi\byte\per\second} of \ac{DDR}-2400\todo{Darf man überhaupt so rechnen?}, the empirically measured bandwidth of a \ac{DMA} is \qty{603}{\mebi\byte\per\second} on a v1A \ac{DPU}.

The \acfi{WRAM} is based on faster but more expensive and less dense static \ac{RAM}.
For this reason, the \ac{WRAM} is far smaller, comprising only \qty{64}{\kibi\byte}, yet its latency is practically zero, and the measured bandwidth for a v1A \ac{DPU} reaches \qty{2688}{\mebi\byte\per\second}.
A typical workflow is, hence, to load input data from the \ac{MRAM} into the \ac{WRAM}, process it, and write output data back into the \ac{MRAM}.
The \ac{WRAM} also contains the stacks of the tasklets, where their local variables are stored.
Global variables which are visible to every tasklet may be stored in the \ac{WRAM} or \ac{MRAM}, however, any \ac{MRAM} variable can be processed only when placing it temporarily in the \ac{WRAM} (see \cref{sec:prereq:arch:isa}).
Unlike a \ac{CPU}, there is no multilevel cache hierarchy with a coherence protocol moving data automatically, and it is in the responsibility of the programmer to ensure that critical data are stored in the \ac{WRAM}.
Still, there is a small number of automatically managed registers.
The driver allows the host to access a specific section of the \ac{WRAM} only if the data has been specifically designated for this purpose, and such transfers are slower than transfers involving the \ac{MRAM}.

Whilst the \ac{WRAM} typically holds the data which is processed, the \acfi{IRAM} contains the program (also called \emph{kernel}) which a \ac{DPU} executes.
The \ac{IRAM} has a size of \qty{24}{\kibi\byte} which translates to a maximum of \num{4096} instructions out of which a kernel has to be built.%
\footnote{
	In fact, a v1B \ac{DPU} can hold \qty{2}{\kibi\byte} of data less in its \ac{MRAM} and 128 instructions fewer in its \ac{IRAM} since parts of those are \textquote{reserved for production and quality control purposes.}~\cite[Introduction~-- DPU chip characteristics]{upmemSDK}
}
This memory can be modified only by the host, as the \ac{DPU} can only read it, which is an automated process usually.

Next to these major memories, there is also a \qty{256}{\bit} large \emph{atomic memory} whose bits are accessible in a thread-safe way, allowing for mutual exclusion, barriers, and similar.
Furthermore, there is a \qty{64}{\bit} large \emph{run memory} through which individual threads can be booted, suspended, and resumed by setting the corresponding status bit.
