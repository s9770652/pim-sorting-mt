\subsection{Structure of a \texorpdfstring{\abb{PIM}}{PIM} Chip}
\label{sec:prereq:arch:structure}

\begin{figure}
	\centering
	\tikzsetnextfilename{arch_chip}
	\begin{tikzpicture}[
		sketch,
		dpu/.style={  },
		mem d/.style={ fill=accentcolor!10!white },
		mem u/.style={ fill=accentcolor!25!white },
		flow/.style={ {Straight Barb[width=1.5mm]}-{Straight Barb[width=1.5mm]} },
		flow left/.style={ {Straight Barb[width=1.5mm]}- },
		flow right/.style={ -{Straight Barb[width=1.5mm]} },
	]
		\def\lenx{24}
		\def\pad{0.5}
		\def\numdpu{4}
		\def\paddpu{0.35}
		\def\paddpuflow{(0.75)}
		\def\intermem{1}
		\def\lendpux{(\lenx-2*\pad-\numdpu*\paddpu)}
		\def\lendpuy{(5)}
		\def\lencontrolx{((\lenx-2*\pad-\intermem)/2)}
		\def\lencontroly{1}
		\def\leny{(\lendpuy+\lencontroly+2*\pad+\numdpu*\paddpu+1)}

		% DPUs.
		%			\draw (0, 0) rectangle +(\lenx, \leny);

		\coordinate (dpu) at (\pad, \pad);
		\draw[dpu] (dpu) rectangle +({\lendpux}, {\lendpuy});
		\foreach \i in {\numdpu,...,0}{
			\pgfmathsetmacro{\fade}{int( (\numdpu+1-\i)/(\numdpu+1) * 100 )}
			\pgfmathsetmacro{\fadefill}{int( (\numdpu+1-\i)/(\numdpu+1) * 0.05 * 100 )}
			\draw[dpu, draw=black!\fade, fill=black!\fadefill] ($(dpu) + (\i*\paddpu, \i*\paddpu)$) coordinate (dpu\i) rectangle +({\lendpux}, {\lendpuy});
		}
		\draw[latex-latex] ($(dpu) +  ({\lendpux+\pad/2}, 0)$) -- +(\numdpu*\paddpu, \numdpu*\paddpu) node[midway, below right=-1mm] {×8};
		\node[above, opacity=0] at (dpu) {×8};

		% Memories.
		\def\padmem{\paddpu}
		\def\lenmemtotalx{(\lendpux-2*\padmem-3*\intermem)}
		\def\lenmramx{(\lenmemtotalx/2)}
		\def\lenmemx{((\lenmemtotalx-\lenmramx)/3)}
		\def\lenmemy{(\lendpuy-2*\padmem)}
		\def\lenmemhalfy{((\lendpuy-2*\padmem-\intermem)/2)}

		\coordinate (mem) at ($(dpu) + (\padmem, \padmem)$);
		\draw[mem u] (mem)                                                        coordinate (pipe) rectangle +( {\lenmemx},     {\lenmemy}) node[midway] {Pipeline};
		\draw[mem u] ($(mem) + ({\lenmemx+\intermem}, 0)$)                        coordinate (wram) rectangle +( {\lenmemx}, {\lenmemhalfy}) node[midway] {WRAM};
		\draw[mem u] ($(mem) + ({\lenmemx+\intermem}, {\lenmemhalfy+\intermem})$) coordinate (iram) rectangle +( {\lenmemx}, {\lenmemhalfy}) node[midway] {IRAM};
		\draw[mem u] ($(mem) + ({2*(\lenmemx+\intermem)}, 0)$)                    coordinate (dma)  rectangle +( {\lenmemx},     {\lenmemy}) node[midway, align=center] {DMA\\Engine};
		\draw[mem d] ($(mem) + ({3*(\lenmemx+\intermem)}, 0)$)                    coordinate (mram) rectangle +({\lenmramx},     {\lenmemy}) node[midway] {MRAM};

		% Data flows.
		\draw[flow]       ($(iram) + (0, {\lenmemhalfy/2})$)          -- +(-\intermem, 0);  % Pipeline ↔ IRAM
		\draw[flow]       ($(wram) + (0, {\lenmemhalfy/2})$)          -- +(-\intermem, 0);  % Pipeline ↔ WRAM
		\draw[flow right] ($(pipe) + ({\lenmemx}, {\lenmemy/2})$)     -- +({2*\intermem+\lenmemx}, 0);  % Pipeline ↔ WRAM

		\draw[flow]       ($(wram) + ({\lenmemx}, {\lenmemhalfy/2})$) -- +(\intermem, 0);  % WRAM ↔ DMA
		\draw[flow left]  ($(iram) + ({\lenmemx}, {\lenmemhalfy/2})$) -- +(\intermem, 0);  % IRAM → DMA

		\draw[flow]       ($(dma)  + ({\lenmemx}, {\lenmemy/2})$)     -- +(\intermem, 0);  % DMA ↔ MRAM

		\draw[flow]       ($({\pad+\lencontrolx}, {\leny-\pad-\lencontroly/2})$)     -- +(\intermem, 0);  % CSI ↔ DDR
		\foreach \i in {\numdpu,...,0}{
			\pgfmathsetmacro{\fade}{int( (\numdpu+1-\i)/(\numdpu+1) * 100 )}
			\def\flowlen{\leny-2*\pad-\lencontroly-\i*\paddpu-\lendpuy+\padmem}
			\draw[flow, draw=black!\fade] ($(dpu\i) + ({\paddpu+\lenmemx/2+\i*\paddpuflow}, {\lendpuy-\padmem})$) -- +($(0, {\flowlen})$);
			\draw[flow, draw=black!\fade] ($(dpu\i) + ({\lendpux-\padmem-\lenmramx/2+\i*\paddpuflow}, {\lendpuy-\padmem})$) -- +($(0, {\flowlen})$);
		}

		% Controllers.
		\draw[mem u] (\pad, {\leny-\pad})                      coordinate (csi) rectangle +({\lencontrolx}, -\lencontroly) node[midway] {Control/Status Interface};
		\draw[mem d] ($(csi) + ({\intermem+\lencontrolx}, 0)$) coordinate (ddr) rectangle +({\lencontrolx}, -\lencontroly) node[midway] {DDR\liningnums{4} Interface};
	\end{tikzpicture}

	\caption{
		The structure of a \ac{PIM} chip.
		The bright components are part of a standard \ac{DDR} package, the dark components are exclusive to \ac{PIM} chips.~\cite{upmem2021WhitePaper}
	}
	\label{fig:arch:chip}
\end{figure}

A \ac{PIM} chip (\cref{fig:arch:chip}) contains eight \ac{DRAM} banks of \qty{64}{\mebi\byte} each.
These are connected with a regular \ac{DDR} interface through which a host \ac{CPU} can access the memory.
Next to each \ac{DRAM} bank, there is a \ac{DPU} which possesses a direct connection to it, thus bypassing the \ac{DDR} interface.
Such an access is also called a \acfi{DMA} and is handled by the \emph{\ac{DMA} engine}.
Furthermore, the eight \acp{DPU} are connected with a special control interface which, in turn, is connected with the memory controller.
This allows the host to communicate with the \acp{DPU} but it does not allow \acp{DPU} to access \ac{DRAM} banks other than their own.
It is not possible for a \ac{DPU} and the host to access a \ac{DRAM} bank concurrently.

\Acp{DPU} contain several major and minor memories.
The memory of the \ac{DRAM} bank is also referred to as \acfi{MRAM}.
It is by far the largest memory of a \ac{DPU} and typically holds both the input provided by the host and the output calculated by the \ac{DPU}.
However, the \ac{MRAM} is also the slowest memory, for each access comes with non-negligible latency.

The \acfi{WRAM} is far smaller, comprising only \qty{64}{\kibi\byte}, yet the latency is practically zero.
A typical workflow is, hence, to load input data from the \ac{MRAM} into the \ac{WRAM}, process it, and write output data back into the \ac{MRAM}.
The \ac{WRAM} also contains the stacks of the tasklets, where their local variables are stored.
Moreover, tasklets can dynamically allocate further space in the \ac{WRAM}.
Global variables which are visible to every tasklet may be stored in the \ac{WRAM} or \ac{MRAM}, however, any \ac{MRAM} variable can be processed only when placing it temporarily in the \ac{WRAM} (see \cref{sec:prereq:arch:isa}).
Unlike a \ac{CPU}, there is no multilevel cache hierarchy with a coherence protocol moving data automatically, and it is in the responsibility of the programmer to ensure that critical data are stored in the \ac{WRAM}.
Still, there is a small number of automatically managed registers (see also \cref{sec:prereq:arch:isa}).
The driver allows the host to access a specific section of the \ac{WRAM} only if the data has been specifically designated for this purpose, and such transfers are slower than transfers involving the \ac{MRAM}.\todo{Wirklich nur aufgrund des Treibers? Da ist ja schließlich noch eine weitere Schnittstelle dazwischen (s. Abb.)}

Whilst the \ac{WRAM} holds the data which is processed, the \acfi{IRAM} contains the program (also called \emph{kernel}) which a \ac{DPU} executes.
The \ac{IRAM} has a size of \qty{24}{\kibi\byte} which translates to a maximum of \num{4096} instructions out of which a kernel has to be built.%
\footnote{
	In fact, a v1B \ac{DPU} can hold \qty{2}{\kibi\byte} of data less in its \ac{MRAM} and 128 instructions fewer in its \ac{IRAM} since parts of those are \textquote{reserved for production and quality control purposes.}~\cite[Introduction~-- DPU chip characteristics]{upmemSDK}
}
This memory can be modified only by the host, as the \ac{DPU} can only read it, which is an automated process usually.

Next to these major memories, there is also a \qty{256}{\bit} large \emph{atomic memory} whose bits are accessible in a thread-safe way, allowing for mutual exclusion, barriers, and similar.
Furthermore, there is a \qty{64}{\bit} large \emph{run memory} through which individual threads can be booted, suspended, and resumed by setting the corresponding status bit.
