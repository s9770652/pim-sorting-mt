\chapter{Parallel Sorting in the MRAM}
\label{sec:par}

A simplistic way to parallelise \MS{} is the following:
Let us assume that the number of tasklets is a power of two and that they are numbered starting from 0.
The whole input array is divided into as many shares of equal length as there are tasklets, and each tasklet sorts a share sequentially using the \ac{MRAM} \MS{} of \cref{sec:mram:merge}.
Once finished, Tasklet \(t\) with \(t \bmod 2 = 1\) informs Tasklet \(t - 1\) that it is finished with sorting its share, and gets permanently suspended.
Tasklet \(t - 1\) merges its original share and the share of Tasklet \(t\) into a bigger run.
Once finished, Tasklet \(t\) with~\(t \bmod 4 = 2\) informs Tasklet \(t - 2\) that it is finished with sorting its share, and gets permanently suspended.
Then, Tasklet \(t - 2\) merges its original share with that of Tasklet \(t\).
This scheme is continued until only two runs remain which get merged by Tasklet~0.

The bottleneck is the sequential execution of each merge which eventually leads to an overall sequentially executed algorithm in the last round.
Even with infinite many processors, this simplistic parallel \MS{} can achieve a theoretical parallel speedup of at most \(\bigtheta{\log n}\).
\Citeauthor{cormen2013algorithmen}~\cite{cormen2013algorithmen} propose an alternative approach whose maximum theoretical parallel speedup is \(\bigtheta{n / \log^2 n}\).
One of its advantage is that the merge procedure does not change fundamentally.
Also, the number of synchronisation points is logarithmic in the number of tasklets only and the time which each synchronisation takes is insignificant compared to the total runtime.

The parallel \MS{} is essentially a full-space \MS{}, meaning it needs an auxiliary array of the same size as the input array and the two arrays switch roles after each round.
The parallel merge procedure (\cref{fig:par:merge}) operates on arbitrary runs, that is, it is no longer demanded that two runs be neighboured when merging.
Likewise, the output location is arbitrary now, too, and not related to the indices of the two runs.
Please note that the algorithm presented here is not stable but will be stabilised later on in this \lcnamecref{sec:par}.
Suppose that there are but two tasklets and both have sorted their respective share of the input.
One of the tasklets is now suspended, whilst the other one determines which of the runs is longer.
Then, it determines the median of the longer run, which will act as \emph{pivot element}.
The pivot is used to find an index~\(i\) which separates the shorter run such that any element with index \(i' < i\) is not greater than the pivot and any element with index \(i' \ge i\) is at least as great as the pivot.
Finding such an index \(i\) can be done through a binary search.
The elements in front of the pivot and index~\(i\), respectively, are at most as great as the pivot so they go in front of the pivot in the output.
This means that the position of the pivot can be calculated by taking the output location and offsetting it by the number of elements within the two front halves.
Now, the front halves of both runs can be merged to the positions in front of the pivot using \cref{alg:mram:two-tier merge}, and the back halves can be merged to the positions behind the pivot.
Since these two merges affect distinct elements, they can be performed in parallel by the two tasklets.

The two tasklets do not merge the same number of elements necessarily, but the workloads cannot differ by a factor greater than three.
The longer run has a length of at least \(n/2\) elements, the shorter run of at most \(n/2\) many.
Due to the pivot being the median of the longer run, both tasklets are guaranteed to merge at least \(n/4\) elements.
In the worst case, the pivot is either strictly less or strictly greater than all elements in the shorter run, meaning the shorter run divided at its beginning or end, respectively, and is merged in its entirety by one of the tasklets.
This means that each tasklet merges at least \qty{25}{\percent} and at most \qty{75}{\percent} of the elements.

The parallel \MS{} can be generalised to work with more than two tasklets.
If there are, for example, four tasklets, then Tasklets~0 and 1 merge their runs in parallel, as do Tasklets~2 and~3.
Then, Tasklet~0 partitions the resulting two runs and assigns two of their halves to Tasklet~2, so that both Tasklet~0 and~2 can partition their particular halves again and merge in parallel with Tasklets~1 and~3, respectively.

\begin{figure}
	\centering
	\tikzsetnextfilename{par_merge}
	\begin{tikzpicture}[
		scale=0.525, thick, line cap=round,  % todo: replace with general styles
		ptr/.style={latex-, node font=\small},  % todo: replace with general styles
		less/.style={very nearly transparent},
		greater/.style={nearly transparent},
		brace/.style={decorate, decoration={calligraphic brace, amplitude=5.5pt, raise=2pt}, line width=1.1pt},
		brace down/.style={brace, decoration={mirror}},
		fade left/.style={dash pattern=on 1pt off 3pt on 1pt off 3pt on 2pt off 3pt on 3pt off 2pt on 12pt},
		fade right/.style={dash pattern=on 12pt off 2pt on 3pt off 3pt on 2pt off 3pt on 1pt off 3pt on 1pt},
		long fade left/.style={dash pattern=on 1pt off 3pt on 1pt off 3pt on 1pt off 3pt on 2pt off 3pt on 2pt off 2pt on 3pt off 2pt on 3pt off 2pt on 14pt},
		long fade right/.style={dash pattern=on 14pt off 2pt on 3pt off 2pt on 3pt off 3pt on 2pt off 3pt on 2pt off 2pt on 1pt off 3pt on 1pt off 3pt on 1pt},
	]
		\def\startA{\pad}
		\def\lenAA{5}
		\def\lenAB{5}

		\def\startB{\padMid+\lenAB+1+\lenAA+\startA}
		\def\lenBA{4}
		\def\lenBB{3}

		\def\startZ{\padMid/2+\pad}

		\def\pad{2}
		\def\padMid{2}
		\def\len{\pad+\lenBB+\lenBA+\padMid+\lenAB+1+\lenAA+\pad}
		\def\distance{3}

		% Arrows.
		\draw[ptr]             (1+\startZ,           1) -- +(90:\distance);
		\draw[rounded corners] (1+\startZ, \distance/3) -| ++(0, 1) -| +(\padMid+\lenAB+\lenAA, \distance/3*2);

		\draw[white, line width=4pt] ({\len-(\startZ)-1}, \distance/3*2+1) -| +(-\lenBA-\padMid-\lenBB, \distance/3);
		\draw[rounded corners]       ({\len-(\startZ)-1},   \distance/3*2) -| ++(0, 1) -| +(-\lenBA-\padMid-\lenBB, \distance/3);
		\draw[ptr]                   ({\len-(\startZ)-1},               1) -- +(90:\distance);

		%			\draw[white, line width=4pt] (\startZ+\lenAA+\lenBA+0.75, 1) to[out=90, in=-90] (0.5+\lenAA+\startA, \distance+1);
		%			\draw[ptr]                   (\startZ+\lenAA+\lenBA+0.50, 1) to[out=90, in=-90] (0.5+\lenAA+\startA, \distance+1);

		\draw[white, line width=4pt] (\startZ+\lenAA+\lenBA+0.45, 1) -| +(0, \distance/4) -- (0.5+\lenAA+\startA, 1+\distance/16*13) -| +(0, \distance/16*3);
		\draw[ptr, rounded corners]  (\startZ+\lenAA+\lenBA+0.50, 1) -| +(0, \distance/4) -- (0.5+\lenAA+\startA, 1+\distance/16*13) -| +(0, \distance/16*3);

		% Filling.
		\fill[   less, accentcolor] (         \startA, 1+\distance) rectangle +(\lenAA, 1);
		\fill[greater, accentcolor] (1+\lenAA+\startA, 1+\distance) rectangle +(\lenAB, 1);

		\fill[   less, accentcolor] (       \startB, 1+\distance) rectangle +(\lenBA, 1);
		\fill[greater, accentcolor] (\lenBA+\startB, 1+\distance) rectangle +(\lenBB, 1);

		\fill[   less, accentcolor] (                \startZ, 0) rectangle +(\lenBA+\lenAA, 1);
		\fill[greater, accentcolor] (1+\lenBA+\lenAA+\startZ, 0) rectangle +(\lenBB+\lenAB, 1);

		% Horizontal lines.
		\draw[fade left]  (        0, 2+\distance) -- +(\pad, 0);
		\draw[fade left]  (        0, 1+\distance) -- +(\pad, 0);
		\draw[fade right] (\len-\pad, 2+\distance) -- +(\pad, 0);
		\draw[fade right] (\len-\pad, 1+\distance) -- +(\pad, 0);

		\draw (\startA, 2+\distance) -- (\len-\pad, 2+\distance);
		\draw (\startA, 1+\distance) -- (\len-\pad, 1+\distance);

		\draw[long fade left]  (               0, 1) -- +(\startZ, 0);
		\draw[long fade left]  (               0, 0) -- +(\startZ, 0);
		\draw[long fade right] ({\len-(\startZ)}, 1) -- +(\startZ, 0);
		\draw[long fade right] ({\len-(\startZ)}, 0) -- +(\startZ, 0);

		\draw (\startZ, 1) -- +({\len-(\startZ)*2}, 0);
		\draw (\startZ, 0) -- +({\len-(\startZ)*2}, 0);

		% Borders.
		\draw (                \startA, 1+\distance) -- +(90:1);
		\draw (         \lenAA+\startA, 1+\distance) -- +(90:1);
		\draw (       1+\lenAA+\startA, 1+\distance) -- +(90:1);
		\draw (\lenAB+1+\lenAA+\startA, 1+\distance) -- +(90:1);

		\draw (              \startB, 1+\distance) -- +(90:1);
		\draw (       \lenBA+\startB, 1+\distance) -- +(90:1);
		\draw (\lenBB+\lenBA+\startB, 1+\distance) -- +(90:1);

		\draw (                              \startZ, 0) -- +(90:1);
		\draw (                \lenBA+\lenAA+\startZ, 0) -- +(90:1);
		\draw (              1+\lenBA+\lenAA+\startZ, 0) -- +(90:1);
		\draw (\lenBB+\lenAB+1+\lenBA+\lenAA+\startZ, 0) -- +(90:1);

		% Braces.
		\draw[brace] (\startA, 2+\distance) --+(\lenAB+1+\lenAA, 0) node[pos=0.5, above=1ex] {longer run\vphantom{p}};
		\draw[brace] (\startB, 2+\distance) --+(  \lenBB+\lenBA, 0) node[pos=0.5, above=1ex] {shorter run\vphantom{p}};

		% Texts.
		\node[align=right, inner sep=0pt, text width=11mm] at (-1.3, 1+\distance+0.5) {\lstinline|Input|};
		\node[align=right, inner sep=0pt]                  at (-1.3,             0.5) {\lstinline|Output|};

		\node at (\startB-\padMid/2, 1+\distance+0.45) {⋯};

		\node at (         \lenAA/2+\startA, 1+\distance+0.5) {\(\le p\)};
		\node at (       0.5+\lenAA+\startA, 1+\distance+0.5) {\(p\vphantom{>}\)};
		\node at (\lenAB/2+1+\lenAA+\startA, 1+\distance+0.5) {\(\ge p\)};

		\node at (       \lenBA/2+\startB, 1+\distance+0.5) {\(\le p\)};
		\node at (\lenBB/2+\lenBA+\startB, 1+\distance+0.5) {\(\ge p\)};

		\node at (                \lenBA/2+\lenAA/2+\startZ, 0.5) {\(\le p\)};
		\node at (                0.5+\lenBA+\lenAA+\startZ, 0.5) {\(p\vphantom{>}\)};
		\node at (\lenBB/2+\lenAB/2+1+\lenBA+\lenAA+\startZ, 0.5) {\(\ge p\)};
	\end{tikzpicture}
	\caption{
		Parallel merge of two runs.
		The first run is the longer one, so its median \(p\) is chosen as pivot which is used to divide the shorter run.
		Afterwards, the pivot is moved to its output location.
		The two front halves of the runs are assigned to one tasklet and, eventually, are written to the positions in front of the pivot.
		At the same time, the two back halves of the runs are processed by another tasklet.
		\cite[Figure~27.6]{cormen2013algorithmen}
	}
	\label{fig:par:merge}
\end{figure}


\paragraph{Communication \& Synchronisation}
The communication network can be visualised as forest of binomial trees.
During the first parallel merge, Tasklet~1 informs Tasklet~0 about being finished with sorting (\emph{bottom-up communication}), and Tasklet~0 informs Tasklet~1 about being finished with partitioning (\emph{top-down communication}).
Likewise, Tasklet~3 and~4 communicate, Tasklets~5 and~6, and so on.
During the second parallel merge, Tasklets~1, 2, and~3 inform Tasklet~0 about being finished with sorting, and Tasklet~0 informs Tasklet~2 about being finished with partitioning.
Then, both tasklets partition again and inform Tasklet~1 and 3, respectively.
This bidirectional scheme is expanded for the third and fourth round of parallel merging if those exist.
Tasklets identify their communication partners through bitwise logic on tasklet identifiers.

To inform others on \emph{which} elements they have to sort, there is a \ac{WRAM} array whither the particular indices are written by the tasklets performing the partitioning.
To inform others on \emph{when} they are finished with sorting or partitioning, tasklets employ \emph{handshakes}.
Handshakes allow for bilateral communication, which is enough for parallel \MS{}.
A tasklet can call \lstinline|handshake_wait_for(id)| and is suspended until Tasklet~\lstinline|id| calls \lstinline|handshake_notify()|.
Likewise, if Tasklet \lstinline|id| calls \lstinline|handshake_notify|, it is suspended until some tasklet calls \lstinline|handshake_wait_for| with its id.
If two or more tasklets wait for the same tasklet, an error is thrown and the execution halts.
Handshakes render bottom-up communication straightforward:
Each non-root calls \lstinline|handshake_notify| when done with sorting, and each root calls \lstinline|handshake_wait_for| for all of its successors.
Due to workload imbalances, some tasklets will try to shake hands earlier than others and have to wait.
Because they are suspended while waiting, they free up the pipeline, thus quickening the pace of the remaining tasklets.
Top-down communication is also straightforward:
After having shaken hands, a non-root immediately calls \lstinline|handshake_notify| again to get suspended once more.
The root repeatedly partitions the runs, writes the division points to the \ac{WRAM} array mentioned earlier, and calls \lstinline|handshake_wait_for| to resume the next tasklet.


\paragraph{Binary Search}
The binary search is conventionally implemented, meaning elements are loaded individually from the \ac{MRAM} and there is no mechanism to load a block of data via \lstinline|mram_read| and search within the \ac{WRAM} once the search interval has been narrowed down enough.
Whilst we did implement such a two-tier binary search, the runtime reduction is below measurement uncertainty.
The reason is that the binary search is executed a few times in total only, so its impact on the total runtime is minuscule.
For the sake of code simplicity and program size, the \ac{WRAM} search tier has been removed.
Reducing the program size is a valid concern since the many unrolled loops bloat the program and only a handful of bytes in the \ac{IRAM} remain free.

Recall that the two halves assigned to a tasklet contain between \qty{25}{\percent} and \qty{75}{\percent} of the elements of the respective runs, which can lead to workload imbalances.
A more even workload of \qty{50}{\percent} would be achieved if the median of the merged run were chosen as pivot and the runs divided accordingly.
Finding the two positions where the runs shall be divided requires a modification to the binary search:
Divide both runs in the middle.
If these are not valid division points, then one of the runs has more little elements in its front half than the other run.
This means that the front half of this run must be longer and the front half of the other run be shorter.
Therefore, the search intervals can be narrowed down to the right side of the run with the less element and to the left side of the run with the greater element.
The process can now be repeated until two valid division points are found.

To our dissatisfaction, we did not manage to implement a bug-free version of this binary search within the timeframe of this thesis, so the effect of load imbalances manifests in our measurements.


\paragraph{Alignment}
In \cref{sec:mram:merge}, it is demanded that all sizes and positions be multiples of 8.
This can be ensured during the first, sequential phase of the parallel \MS{} by dividing the input accordingly and introducing dummy variables if need be.
In the second phase with parallel merging, there is no control over any alignment whatsoever because the sizes of run halves are arbitrary.
This raises the need for modifications to the sequential merge procedure, which slow it down if the input consists of 32-bit elements.

Before beginning with the first tier, the alignment of the output location must be checked.
If it is unaligned, the less of the head elements is written to the output location.
Now, the output location is aligned to 8 bytes again, meaning the first tier can proceed as normal since emptying the cache through \lstinline|mram_write| is unproblematic.

At first, the second tier proceeds as normal, too.
Once the less run is depleted, the cache may contain an odd number of elements, so the current element of the greater run is written to the cache before emptying it.
However, if there are elements still remaining in the greater run, flushing the remainder becomes more complicated than in \cref{sec:mram:merge}, where it was sufficient to loop over the remainder in the \ac{MRAM}, write it to the cache, and move it to the respective output location.
The current output location is still aligned to 8 bytes but it may very well be that the first element of the remainder has an unaligned address.
This indicates a mismatch, for all unaligned elements must be transferred to an aligned address and all aligned elements to an unaligned one.
When using \lstinline|mram_read| and \lstinline|mram_write|, the alignment of elements within the \ac{MRAM} and within the \ac{WRAM} is the same.
In such an instance, the remainder is loaded blockwise from the \ac{MRAM} into the cache.
There, a loop shifts each element forward by one position, resolving the mismatch.
Afterwards, the shifted elements can be written to the output location.
Since only an even number of elements can be moved via \lstinline|mram_write|, it may be necessary to transfer a single item individually at the end in case that the remainder had an odd length.

Writing single 32-bit elements to the \ac{MRAM} is not threadsafe, since, internally, eight bytes are read to an oblique \ac{WRAM} cache, partially modified, and written back to the \ac{MRAM}.
Therefore, a so-called atomic write, which utilises costly virtual mutexes, must be performed.


\paragraph{Stability}
The parallel merging algorithm as presented above makes \MS{} unstable.
The reason is that one or both runs may contain the pivot value multiple times and that a division point separates the duplicates.
Therefore, it must be ensured that all duplicates of a run remain within the same half.
To do so, it is checked if its left neighbour has the same value once the value of the median is determined.
If so, there may be even more duplicates so a binary search is employed to find the first occurrence of the pivot value within the longer run.
This first occurrence marks the division point for the longer run.
Then, it must be checked whether the shorter run precedes or follows the longer run.
If it precedes it, then possible duplicates of the pivot must also precede the counterparts in the longer run.
In such an instance, a binary search is employed to find the greatest index \(i\) of the shorter run such that the element with index \(i - 1\) is at most as great as the pivot.
If, however, the shorter run follows the longer run, analogous reasoning dictates the employment of a binary search to find the least index \(i\) such that the element with index \(i\) is at least as great as the pivot.
In any case, it is now known how many elements precede the pivot so its position in the output can be determined.

To our great dissatisfaction, we did not manage to implement a bug-free version for 16 tasklets within the timeframe of this thesis, as edge cases with many empty pairs of runs lead to buffer overflows with unknown sources.



\subsection{Evaluation of the Performance}

The parallel speedup of \MS{} for \qty{32}{\mebi\byte} of data is shown in \cref{fig:par:speedup}.
The optimal parallel speedup is linear in the number of tasklets but is capped at 11 or, rather, slightly above because of \ac{DMA} latency hiding.
For uniformly distributed inputs and inputs following Zipf's law, the measured speedup is very close to the optimum, reaching values above 10 for both 32-bit and 64-bit integers.
This is owed to workloads tending to be balanced naturally and tasklets being removed from the pipeline once they are finished.
For all other inputs, the parallel speedup is roughly between 7 and 9 with 32-bit integers and between~6 and~8 with 64-bit integers \Dash a consequence of workloads becoming more unbalanced.
This shall be illustrated by sorted inputs:
In the last round, two runs remain.
When Tasklet 0 performs the first partitioning step, the longer run is cut exactly in half because of the pivot being the median.
However, the pivot is strictly less or greater than any element in the shorter run, meaning the shorter run keeps its size of about \(n/2\) many elements as the division point is at one of its ends.
The ratio between the least and the greatest number of assigned elements in the last round of parallel merging is about 2.3 for zero-one inputs and 4 for the three kinds of sorted inputs.
A modified binary search as described previously would probably help bringing these little parallel speedups on par with those of the uniform and Zipf's input distribution.

\expandafter\pgfplotsinvokeforeach\expandafter{\alldists}{
	% 32-bit | Instabil
	\pgfplotstablenewnamed[create on use/n/.style={}, create on use/µ_MergePar/.style={}, create on use/σ_MergePar/.style={}, columns={n,µ_MergePar,σ_MergePar}]{0}{tableMergeParUnstable_32#1}
	\pgfplotstablevertcatnamed{tableMergeParUnstable_32#1}{data/merge_par/NR_TASKLETS=1/STABLE=false/uint32/#1.txt}
	\pgfplotstablevertcatnamed{tableMergeParUnstable_32#1}{data/merge_par/NR_TASKLETS=2/STABLE=false/uint32/#1.txt}
	\pgfplotstablevertcatnamed{tableMergeParUnstable_32#1}{data/merge_par/NR_TASKLETS=4/STABLE=false/uint32/#1.txt}
	\pgfplotstablevertcatnamed{tableMergeParUnstable_32#1}{data/merge_par/NR_TASKLETS=8/STABLE=false/uint32/#1.txt}
	\pgfplotstablevertcatnamed{tableMergeParUnstable_32#1}{data/merge_par/NR_TASKLETS=16/STABLE=false/uint32/#1.txt}

	% 64-bit | Instabil
	\pgfplotstablenewnamed[create on use/n/.style={}, create on use/µ_MergePar/.style={}, create on use/σ_MergePar/.style={}, columns={n,µ_MergePar,σ_MergePar}]{0}{tableMergeParUnstable_64#1}
	\pgfplotstablevertcatnamed{tableMergeParUnstable_64#1}{data/merge_par/NR_TASKLETS=1/STABLE=false/uint64/#1.txt}
	\pgfplotstablevertcatnamed{tableMergeParUnstable_64#1}{data/merge_par/NR_TASKLETS=2/STABLE=false/uint64/#1.txt}
	\pgfplotstablevertcatnamed{tableMergeParUnstable_64#1}{data/merge_par/NR_TASKLETS=4/STABLE=false/uint64/#1.txt}
	\pgfplotstablevertcatnamed{tableMergeParUnstable_64#1}{data/merge_par/NR_TASKLETS=8/STABLE=false/uint64/#1.txt}
	\pgfplotstablevertcatnamed{tableMergeParUnstable_64#1}{data/merge_par/NR_TASKLETS=16/STABLE=false/uint64/#1.txt}
}

\NewDocumentCommand{\speeduppar}{m}{
	\pgfplotstablegetelem{0}{µ_MergePar}\of#1
	\pgfmathsetmacro{\messlatte}{\pgfplotsretval}
		\addplot+ table[
		create on use/tasklets/.style={create col/set list={1,2,4,8,16}},
		create on use/factor/.style={create col/expr={\messlatte / \thisrow{µ_MergePar}}},
		x=tasklets, y=factor
	] {#1};
}

\begin{figure}[p]
	\tikzsetnextfilename{speedup}
	\begin{tikzpicture}[plot]
		\begin{groupplot}[
			adaptive group=1 by 2,
			groupplot xlabel={Tasklets},
			groupplot ylabel={Parallel Speedup},
			xtick={1,2,4,8,16},
			xmode=log,
			ymin=0,
			ymax=12,
			ytick distance=2,
		]
			\nextgroupplot[title/.add={}{32-bit}]
			\pgfplotsset{legend to name=leg:par:speedup, legend entries={Sorted, Reverse S., Almost S., Zero-One, Uniform, Zipf's}}
			\addplot[forget plot, very nearly transparent] coordinates {(1,1)(2,2)(3,3)(4,4)(5,5)(6,6)(7,7)(8,8)(9,9)(10,10)(11,11)(16,11)};
			\expandafter\pgfplotsinvokeforeach\expandafter{\alldists}{
				\expandafter\speeduppar\expandafter{\csname tableMergeParUnstable_32#1\endcsname}
			}
			%
			\nextgroupplot[title/.add={}{64-bit}]
			\addplot[forget plot, very nearly transparent] coordinates {(1,1)(2,2)(3,3)(4,4)(5,5)(6,6)(7,7)(8,8)(9,9)(10,10)(11,11)(16,11)};
			\expandafter\pgfplotsinvokeforeach\expandafter{\alldists}{
				\expandafter\speeduppar\expandafter{\csname tableMergeParUnstable_64#1\endcsname}
			}
		\end{groupplot}
	\end{tikzpicture}

	\hfil\pgfplotslegendfromname{leg:par:speedup}\hfil
	\caption{
		Mean parallel speedup of \MS{} on all benchmarked input distributions and data types with \qty{32}{\mebi\byte} of data.
		The grey line indicates an idealistic, linear speedup which is capped at 11.
	}
	\label{fig:par:speedup}
\end{figure}

\Cref{fig:par:shares} shows the total runtimes of the parallel \MS{}.
The measurements are subdivided into the three phases of the parallel \MS{}, that is the sequential \ac{WRAM} phase, the sequential \ac{MRAM} phase, and the parallel \ac{MRAM} phase.
For the most part, the results are unsurprising.
The sorted, reverse sorted, and zero-one input distribution are sorted quickly as they lead to a short first and second phase, whilst the uniform and Zipf's input distribution lead to longer runtimes as these two phases are longer.
The third phase always takes roughly the same amount of time, implying that workload imbalances are cancelled out by earlier flushes, although the effect is weaker for 64-bit integers where computation is more costly.
The greater parallel speedup of the uniform and Zipf's input distribution shines through by the third phase being shorter compared to the relatively long first and second phase.

Almost sorted inputs, on the other hand, are clear outsiders because of the remarkably long third phase, making them the worst case amongst all six benchmarked ones.
Despite the long runtime, the parallel speedup is about the same as for sorted and reverse sorted inputs.
Indeed, they all share equal or nearly equal workload imbalances.
The explanation for the third phase being so long is the same as the one given in \cref{sec:mram:merge:performance} with respect to the second phase.
A few great elements placed in a run of mostly little elements delays flushes, and the longer the runs become, the likelier it is for a run to contain an overly great element.

\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergePar/.style={}, create on use/σ_MergePar/.style={}, columns={n,µ_MergePar,σ_MergePar}]{0}{\tableMergeParFirst}
\expandafter\pgfplotsinvokeforeach\expandafter{\alldists}{
	\pgfplotstablevertcat{\tableMergeParFirst}{data/merge_par/phase_1/STABLE=false/uint32/#1.txt}
}
\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergeFS/.style={}, create on use/σ_MergeFS/.style={}, columns={n,µ_MergeFS,σ_MergeFS}]{0}{\tableMergeParSec}
\expandafter\pgfplotsinvokeforeach\expandafter{\alldists}{
	\pgfplotstablevertcat{\tableMergeParSec}{data/merge_mram/NR_TASKLETS=16/CACHE_SIZE=1024/SEQREAD_CACHE_SIZE=512/uint32/#1.txt}
}
\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergePar/.style={}, create on use/σ_MergePar/.style={}, columns={n,µ_MergePar,σ_MergePar}]{0}{\tableMergeParThird}
\expandafter\pgfplotsinvokeforeach\expandafter{\alldists}{
	\pgfplotstablevertcat{\tableMergeParThird}{data/merge_par/NR_TASKLETS=16/STABLE=false/uint32/#1.txt}
}
\pgfplotstablenew[
	create on use/dist/.style={create col/set list={Sorted,Reverse,Almost,Zero-One,Uniform,Zipf's}},
	create on use/first/.style={create col/copy column from table={\tableMergeParFirst}{µ_MergePar}},
	create on use/second/.style={create col/copy column from table={\tableMergeParSec}{µ_MergeFS}},
	create on use/third/.style={create col/copy column from table={\tableMergeParThird}{µ_MergePar}},
	columns={dist,first,second,third},
]{6}{\tableMergeMramXxxii}

\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergePar/.style={}, create on use/σ_MergePar/.style={}, columns={n,µ_MergePar,σ_MergePar}]{0}{\tableMergeParFirst}
\expandafter\pgfplotsinvokeforeach\expandafter{\alldists}{
	\pgfplotstablevertcat{\tableMergeParFirst}{data/merge_par/phase_1/STABLE=false/uint64/#1.txt}
}
\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergeFS/.style={}, create on use/σ_MergeFS/.style={}, columns={n,µ_MergeFS,σ_MergeFS}]{0}{\tableMergeParSec}
\expandafter\pgfplotsinvokeforeach\expandafter{\alldists}{
	\pgfplotstablevertcat{\tableMergeParSec}{data/merge_mram/NR_TASKLETS=16/CACHE_SIZE=1024/SEQREAD_CACHE_SIZE=512/uint64/#1.txt}
}
\pgfplotstablenew[create on use/n/.style={}, create on use/µ_MergePar/.style={}, create on use/σ_MergePar/.style={}, columns={n,µ_MergePar,σ_MergePar}]{0}{\tableMergeParThird}
\expandafter\pgfplotsinvokeforeach\expandafter{\alldists}{
	\pgfplotstablevertcat{\tableMergeParThird}{data/merge_par/NR_TASKLETS=16/STABLE=false/uint64/#1.txt}
}
\pgfplotstablenew[
	create on use/dist/.style={create col/set list={Sorted,Reverse,Almost,Zero-One,Uniform,Zipf's}},
	create on use/first/.style={create col/copy column from table={\tableMergeParFirst}{µ_MergePar}},
	create on use/second/.style={create col/copy column from table={\tableMergeParSec}{µ_MergeFS}},
	create on use/third/.style={create col/copy column from table={\tableMergeParThird}{µ_MergePar}},
	columns={dist,first,second,third},
]{6}{\tableMergeMramLxiv}

\begin{figure}
	\tikzsetnextfilename{shares}
	\begin{tikzpicture}[plot]
		\begin{groupplot}[
			adaptive group=1 by 2,
			groupplot xlabel={},
			groupplot ylabel={Cycles},
			enlarge x limits={abs=6mm},
			xtick style={draw=none},
			symbolic x coords={Sorted,Reverse,Almost,Zero-One,Uniform,Zipf's},
			x tick label style={
				rotate=90,
				anchor=east,
			},
			ybar=-\pgfkeysvalueof{/pgf/bar width},
			ymin=0,
			ytick scale label code/.code={},
			xmajorgrids=false,
		]
			\nextgroupplot[ymax=1.5e9, title/.add={}{32-bit}]
			\pgfplotsset{legend to name=leg:par:shares, legend entries={Third Phase,Second Phase,First Phase}, legend reversed}
			\pgfplotsset{cycle list shift=2}
			\addplot+ table[x=dist, y=third] {\tableMergeMramXxxii};
			\pgfplotsset{cycle list shift=0}
			\addplot+ table[x=dist, y=second] {\tableMergeMramXxxii};
			\pgfplotsset{cycle list shift=-2}
			\addplot+ table[x=dist, y=first] {\tableMergeMramXxxii};
			%
			\nextgroupplot[ymax=1e9, title/.add={}{64-bit}]
			\pgfplotsset{cycle list shift=2}
			\addplot+ table[x=dist, y=third] {\tableMergeMramLxiv};
			\pgfplotsset{cycle list shift=0}
			\addplot+ table[x=dist, y=second] {\tableMergeMramLxiv};
			\pgfplotsset{cycle list shift=-2}
			\addplot+ table[x=dist, y=first] {\tableMergeMramLxiv};
		\end{groupplot}
	\end{tikzpicture}

	\hfil\pgfplotslegendfromname{leg:par:shares}\hfil
	\caption{
		Mean runtimes of the parallel \MS{}, broken down into its three phases, on all benchmarked input distributions and data types with \qty{32}{\mebi\byte} of data.
		The first phase comprises sequential sorting in the \ac{WRAM}, the second one sequential sorting in the \ac{MRAM}, and the third one parallel sorting in the \ac{MRAM}.
	}
	\label{fig:par:shares}
\end{figure}
